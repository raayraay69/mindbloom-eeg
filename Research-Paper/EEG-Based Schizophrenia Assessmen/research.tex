\documentclass[10pt]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\usepackage{float}
\usepackage{enumitem}
\usepackage{times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}

% Page style for preprint header
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\footnotesize A PREPRINT}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}

% ============================================================================
% TITLE AND AUTHORS
% ============================================================================
\title{\textsc{A System-Level Framework for EEG-Based Schizophrenia Assessment: Methodological Rigor, Uncertainty Quantification, and Hardware Feasibility}}

\author[1]{Samiksha B. Chandrasekaran}
\author[1]{Eric Raymond}
\affil[1]{Department of Computer Science, Indiana University South Bend, South Bend, IN 46615, USA}
\affil[1]{Purdue University Indianapolis, Indianapolis, IN 46202, USA}

\date{}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Schizophrenia diagnosis remains predominantly subjective, relying on clinical interviews and behavioral observation, while machine learning approaches using electroencephalography (EEG) have shown promise for objective biomarker discovery. Many published studies suffer from methodological flaws, particularly ``subject leakage,'' where recordings from the same individual contaminate both training and testing sets and artificially inflate reported accuracies. This work presents a rigorously validated EEG classification pipeline that prioritizes methodological integrity over inflated performance metrics. Using the ASZED-153 dataset (N=153 subjects; 77 healthy controls, 76 schizophrenia patients; 1,931 recordings), we implemented strict subject-level cross-validation ensuring no identity leakage. Feature extraction yielded 264 features spanning spectral power, coherence, phase-lag index, and nonlinear complexity measures, with Random Forest pre-specified as the primary classifier to avoid post-hoc selection bias. Subject-level classification achieved 83.7\% accuracy (95\% CI: 77.8--89.5\%) with ROC-AUC of 0.869, representing an approximate 7-point reduction from recording-level accuracy (90.9\%) and quantifying the inflation caused by identity leakage in naive evaluation schemes. All classification results were obtained from research-grade EEG equipment; the ESP32-based hardware prototype presented here serves as a proof-of-concept for future validation studies. Feature importance analysis revealed frontal channels (Fp1, Fp2) as top predictors, providing biological rationale for targeting frontal sites in future low-cost hardware designs. By transparently reporting honest metrics obtained through rigorous methodology, this work establishes a reproducible baseline for EEG-based schizophrenia screening and offers a pathway toward accessible psychiatric assessment tools for underserved populations. % REVISION: Reframed hardware as proof-of-concept, clarified classification used research-grade EEG
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Schizophrenia affects approximately 1\% of the global population, with diagnosis typically occurring years after symptom onset due to the disorder's heterogeneous presentation and the absence of objective biomarkers \cite{who2022}. Current diagnostic practice relies heavily on clinical interviews and behavioral observation, introducing subjectivity that can delay treatment initiation and contribute to poor long-term outcomes \cite{tandon2013}.

The application of machine learning (ML) to electroencephalography (EEG) has emerged as a promising avenue for developing objective, quantitative markers of schizophrenia \cite{murphy2021, phang2020}. EEG offers several practical advantages: it is non-invasive, relatively inexpensive, and captures the neural oscillatory dynamics known to be disrupted in psychotic disorders \cite{uhlhaas2010}. Published studies have reported classification accuracies exceeding 90\%, suggesting that EEG-based diagnostic tools may soon augment clinical practice.

The ML-for-health literature, however, faces a well-documented replication crisis \cite{roberts2021, varoquaux2022}. A substantial proportion of studies suffer from methodological flaws that artificially inflate reported performance. Most problematic is \textit{identity leakage}, the contamination of test sets with recordings from subjects present in the training set. When multiple recordings per subject exist (as is common in EEG datasets), naive random splitting allows the model to exploit subject-specific patterns (voice, electrode impedance, head shape) rather than learning disorder-relevant biomarkers. The resulting accuracy estimates are overly optimistic and fail to generalize to new individuals \cite{little2017, saeb2017}.

\subsection{Contributions of This Work}

This paper makes four primary contributions:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Rigorous Evaluation Protocol:} We implement strict subject-level stratified cross-validation, ensuring that all recordings from a given individual remain entirely within either the training or testing fold, never split between them. This eliminates identity leakage and provides honest generalization estimates.

    \item \textbf{Transparent Quantification of Methodological Impact:} We explicitly compare recording-level accuracy (the inflated metric) with subject-level accuracy (the honest metric), quantifying the ``cost of rigor'' at approximately 7 percentage points.

    \item \textbf{Pre-Specified Analysis Plan:} The primary classifier (Random Forest) was designated before evaluation to prevent post-hoc model selection bias, adhering to principles of registered reporting.

    \item \textbf{Hardware Feasibility Demonstration:} We developed a low-cost (\$50) single-channel EEG proof-of-concept prototype based on the ESP32 microcontroller and BioAmp EXG Pill. Feature importance analysis from research-grade data suggests that frontal channels carry sufficient discriminative information, providing biological rationale for this design. Validation with hardware-acquired signals remains future work. % REVISION: Clarified hardware is proof-of-concept, not validated
\end{enumerate}

This work prioritizes \textit{methodological integrity} over state-of-the-art accuracy claims. The resulting 83.7\% subject-level accuracy, while modest compared to inflated literature benchmarks, represents an honest, reproducible baseline upon which future work can build. All classification results reported here were obtained from research-grade EEG equipment; the hardware prototype serves as a proof-of-concept for future accessibility-focused validation studies. % REVISION: Added explicit statement about research-grade EEG

% ============================================================================
% 2. MATERIALS AND METHODS
% ============================================================================
\section{Materials and Methods}

\subsection{Dataset: ASZED-153}

We utilized the African Schizophrenia EEG Dataset (ASZED), version 1.1, publicly available through Zenodo (DOI: 10.5281/zenodo.14178398) \cite{aszed2024}. This dataset comprises EEG recordings from 153 participants: 77 healthy controls (HC) and 76 patients with schizophrenia (SZ), recruited from two clinical sites in Nigeria. Demographic characteristics are summarized in Table~\ref{tab:demographics}.

Recordings were acquired using a 16-channel montage following the international 10-20 system (Fp1, Fp2, F3, F4, C3, C4, P3, P4, O1, O2, F7, F8, T3, T4, T5, T6) at 256 Hz sampling rate. Multiple paradigms were recorded per subject, including resting-state (eyes open/closed), cognitive tasks, mismatch negativity (MMN), and auditory steady-state response (ASSR), yielding a total of 1,931 usable recordings (mean: 12.6 recordings per subject).

\begin{table}[t]
\centering
\caption{Demographic Characteristics of the ASZED-153 Dataset}
\label{tab:demographics}
\begin{tabular}{lcc}
\toprule
\textbf{Characteristic} & \textbf{Controls (HC)} & \textbf{Patients (SZ)} \\
\midrule
N (subjects) & 77 & 76 \\
N (recordings) & 990 & 941 \\
Recordings/subject & 12.9 & 12.4 \\
Recording sites & \multicolumn{2}{c}{2 (Nigeria)} \\
Sampling rate & \multicolumn{2}{c}{256 Hz} \\
Channels & \multicolumn{2}{c}{16 (10-20 system)} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Preprocessing Pipeline}

All preprocessing was implemented in Python 3.10 using MNE-Python \cite{gramfort2013}. The pipeline comprised:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Channel Standardization:} Raw channel names (e.g., ``Fp1[1]'') were canonicalized to standard 10-20 nomenclature. Missing channels were zero-padded in-place to maintain consistent feature indexing.

    \item \textbf{Filtering:} Fourth-order Butterworth bandpass filter (0.5-45 Hz) followed by a 50 Hz notch filter (Nigeria power grid frequency).

    \item \textbf{Quality Control:} Files with fewer than 10 matched channels or fewer than 500 samples were rejected. Rejection rates were monitored for selection bias using Fisher's exact test.

    \item \textbf{Resampling:} All recordings were resampled to 250 Hz for computational consistency.
\end{enumerate}

Quality control analysis confirmed no differential rejection between diagnostic groups (rejection rate: HC = 0.1\%, SZ = 0.0\%; Fisher exact $p = 1.0$), ensuring that preprocessing did not introduce selection bias.

\subsection{Feature Extraction}

We extracted 264 features per recording, organized into six categories:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Spectral Power (80 features):} Band power computed via Welch's method for delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-13 Hz), beta (13-30 Hz), and gamma (30-45 Hz) across all 16 channels.

    \item \textbf{ERP-like Components (20 features):} ERP-like temporal dynamics computed on the Global Field Power (GFP; spatial RMS across all 16 channels) in windows corresponding to N100 (80-120ms), P200 (150-250ms), MMN (150-200ms), and P300 (250-400ms) latencies. Features include peak amplitude, peak latency, window mean, standard deviation, and area under curve. % REVISION: Clarified GFP-based computation and added latency windows

    \item \textbf{Inter-channel Coherence (30 features):} Magnitude-squared coherence between six electrode pairs across five frequency bands. Pairs were chosen to capture anterior-posterior connectivity (Fp1-O1, Fp2-O2), interhemispheric frontal and occipital connectivity (Fp1-Fp2, O1-O2), and motor-temporal connectivity (C3-T3, C4-T4), regions implicated in schizophrenia-related dysconnectivity. % REVISION: Added electrode pair rationale

    \item \textbf{Phase-Lag Index (6 features):} PLI computed for the same six electrode pairs in the alpha band (8-13 Hz), chosen because alpha-band synchronization abnormalities are well-documented in schizophrenia. PLI quantifies phase synchronization while minimizing volume conduction artifacts \cite{stam2007}. % REVISION: Added frequency band specification (alpha)

    \item \textbf{Statistical Moments (96 features):} Mean, standard deviation, skewness, kurtosis, RMS, and peak-to-peak amplitude for each channel.

    \item \textbf{Nonlinear Complexity (32 features):} Sample entropy and Higuchi fractal dimension for each channel, capturing signal complexity reductions associated with schizophrenia \cite{kim2000}.
\end{enumerate}

% REVISION: Added feature dimensionality justification paragraph
\paragraph{Justification for Feature Dimensionality.} The 264-dimensional feature space with 153 subjects ($p > n$) requires methodological justification. Random Forest classifiers are well-suited to high-dimensional settings for several reasons: (1) bagging introduces implicit regularization by training each tree on a bootstrap sample, (2) random feature subsampling at each split prevents any single feature from dominating, and (3) the \texttt{max\_depth=20} constraint limits tree complexity, reducing overfitting risk. We deliberately avoided explicit feature selection (e.g., mutual information filtering, recursive feature elimination) prior to cross-validation, as such selection applied to the full dataset would introduce selection bias and inflate performance estimates. The subject-level cross-validation ensures that any feature importance patterns learned reflect generalizable signal rather than training-set-specific noise.

\subsection{Strict Subject-Level Cross-Validation}
\label{sec:cv}

To prevent identity leakage, we implemented a strict subject-level evaluation protocol: % REVISION: Removed fig:cv_diagram reference (figure not included); procedure described inline below

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Subject-Level Stratification:} Five-fold stratified splitting was performed on the 153 \textit{subjects} (not recordings), ensuring approximately equal class proportions in each fold.

    \item \textbf{Fold Expansion:} Subject-level folds were then expanded to include all recordings belonging to each subject, guaranteeing that no subject appeared in both training and testing partitions.

    \item \textbf{Within-Fold Normalization:} Feature standardization (z-scoring via StandardScaler) was applied \textit{inside} each fold, fitting only on training data to prevent information leakage.

    \item \textbf{Subject-Level Aggregation:} Test predictions were aggregated by subject using mean probability voting, with final classification determined at threshold 0.5.
\end{enumerate}

This protocol ensures that reported metrics reflect true generalization to \textit{new individuals}, not memorization of subject-specific artifacts.

% REVISION: Added threshold and CV fold justification
\paragraph{Methodological Design Choices.} Two design decisions require justification: (1) \textit{Decision threshold}: the 0.5 probability threshold was used as the scikit-learn default; threshold optimization was deliberately avoided to prevent overfitting to the validation data. In deployment, threshold adjustment based on clinical priorities (e.g., maximizing sensitivity) would be appropriate but should be performed on held-out data. (2) \textit{Number of folds}: five-fold CV was pre-specified to balance training set size ($\sim$122 subjects, sufficient for Random Forest training) with number of independent test evaluations. Fewer folds would reduce test variance but limit training data; more folds would increase computational cost with diminishing returns.

\subsection{Classification Models}

Four classifiers were evaluated:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Random Forest (RF):} 300 trees, max depth 20, min samples split 5. Designated as the \textit{primary model} before analysis.
    \item \textbf{Logistic Regression (LR):} L2 regularization, max 1000 iterations.
    \item \textbf{Gradient Boosting (GB):} 100 estimators, default hyperparameters.
    \item \textbf{Support Vector Machine (SVM):} RBF kernel, probability calibration enabled.
\end{itemize}

All models were wrapped in scikit-learn Pipelines to ensure proper scaler fitting within each CV fold \cite{pedregosa2011}.

\subsection{Hardware Prototype (Proof-of-Concept)} % REVISION: Renamed section to clarify status

To demonstrate \textit{design feasibility} for low-resource settings, we developed a proof-of-concept single-channel EEG acquisition system comprising: % REVISION: Changed "feasibility" to "design feasibility" and added "proof-of-concept"

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Microcontroller:} ESP32 (dual-core, Wi-Fi/Bluetooth enabled, \$5)
    \item \textbf{Analog Front-End:} BioAmp EXG Pill (instrumentation amplifier with $\times$1000 gain, \$25)
    \item \textbf{Electrodes:} Dry Ag/AgCl electrodes at Fp1 position
    \item \textbf{Sampling:} 256 Hz, 12-bit ADC resolution
\end{itemize}

Total hardware cost was approximately \$50 USD, compared to \$5,000-50,000 for clinical-grade EEG systems. This prototype represents a proof-of-concept design motivated by feature importance analysis. All classification results reported in this paper were obtained from research-grade 16-channel EEG equipment; validation of classification performance using hardware-acquired signals remains an essential direction for future work. % REVISION: Added explicit disclaimer about hardware validation status

% ============================================================================
% 3. RESULTS
% ============================================================================
\section{Results}

\subsection{Quality Control and Selection Bias Analysis}

Of 1,932 raw EEG files in the dataset, 1,931 (99.95\%) passed quality control and were retained for analysis. The single rejection was due to insufficient recording length (<500 samples). Crucially, rejection rates did not differ between diagnostic groups (HC: 0.1\%, SZ: 0.0\%; Fisher exact test $p = 1.0$), confirming that preprocessing did not introduce selection bias. % REVISION: Clarified 1,932 raw â†’ 1,931 after QC to resolve apparent discrepancy with dataset section

All 153 subjects were retained for analysis, with matched channel counts (mean = 16.0, range: 16--16) indicating successful channel canonicalization.

\subsection{Classification Performance}

Table~\ref{tab:results} presents classification performance across all models. Random Forest, the pre-specified primary model, achieved 83.7\% subject-level accuracy (95\% CI: 77.8--89.5\%) with ROC-AUC of 0.869.

\begin{table}[t]
\centering
\caption{Classification Performance (Subject-Level, N=153)} % REVISION: Enhanced with CIs, baseline, and model comparison footnote
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{AUC (95\% CI)} & \textbf{F1 (95\% CI)} \\
\midrule
Majority-class baseline$^\dagger$ & 50.3 & 0.500 & --- \\
\midrule
Logistic Regression & 76.5 (69.4--83.6) & 0.811 (0.74--0.88) & 0.768 (0.69--0.85) \\
SVM (RBF) & 81.7 (75.2--88.2) & 0.852 (0.79--0.91) & 0.823 (0.75--0.89) \\
Gradient Boosting & 83.7 (77.8--89.5) & 0.871 (0.81--0.93) & 0.837 (0.77--0.90) \\
Random Forest$^*$ & \textbf{83.7 (77.8--89.5)} & \textbf{0.869 (0.81--0.93)} & \textbf{0.837 (0.77--0.90)} \\
\bottomrule
\multicolumn{4}{l}{\footnotesize $^*$Pre-specified primary model. $^\dagger$Always predicts HC (majority class: 77/153 = 50.3\%).} \\
\multicolumn{4}{l}{\footnotesize Note: No pairwise model comparisons performed; overlapping CIs suggest} \\
\multicolumn{4}{l}{\footnotesize comparable performance among RF, GB, and SVM.} \\
\end{tabular}
\end{table}

The subject-level confusion matrix for Random Forest is presented in Table~\ref{tab:confusion}. Note that all cells sum to N=153 subjects, not recordings.

\begin{table}[t]
\centering
\caption{Confusion Matrix (Subject-Level, Random Forest)}
\label{tab:confusion}
\begin{tabular}{lcc}
\toprule
& \textbf{Predicted HC} & \textbf{Predicted SZ} \\
\midrule
\textbf{Actual HC (n=77)} & 57 & 20 \\
\textbf{Actual SZ (n=76)} & 5 & 71 \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\footnotesize
Sensitivity (SZ recall) = 93.4\%, Specificity (HC recall) = 74.0\%
\end{table}

The model demonstrated high sensitivity (93.4\%) for detecting schizophrenia cases, at the cost of moderate specificity (74.0\%). This trade-off is appropriate for a screening tool where missing true cases carries greater clinical cost than false positives.

% REVISION: Added clinical utility analysis (PPV/NPV at realistic prevalence)
\paragraph{Clinical Utility at Population Prevalence.} Sensitivity and specificity alone are insufficient for clinical deployment decisions; predictive values at realistic base rates are essential. At the general population prevalence of $\sim$1\%, applying Bayes' theorem yields:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Positive Predictive Value (PPV):} $\frac{0.934 \times 0.01}{0.934 \times 0.01 + 0.26 \times 0.99} = 3.5\%$
    \item \textbf{Negative Predictive Value (NPV):} $\frac{0.74 \times 0.99}{0.74 \times 0.99 + 0.066 \times 0.01} = 99.9\%$
\end{itemize}

The 3.5\% PPV at general population prevalence is notably low, meaning approximately 97\% of positive screens would be false positives. This underscores that EEG-based screening is not suitable for general population deployment. The 99.9\% NPV, though, suggests excellent rule-out capability: a negative screen provides strong reassurance. Clinical utility improves substantially in enriched settings: at 10\% prevalence (e.g., first-degree relatives of patients, or individuals presenting with prodromal symptoms), PPV rises to 28.5\%, and at 30\% prevalence (e.g., psychiatric outpatient clinics), PPV reaches 57.2\%. These calculations demonstrate that the tool is most appropriate as a screening aid in high-risk populations or as a triage mechanism to prioritize clinical evaluation, not as a standalone diagnostic.

\subsection{Quantifying Identity Leakage}

To quantify the inflation caused by identity leakage, we compared recording-level and subject-level accuracy (Table~\ref{tab:leakage}). Recording-level accuracy (90.9\%) exceeded subject-level accuracy (83.7\%) by 7.2 percentage points, representing the ``hidden cost'' of naive evaluation protocols.

\begin{table}[t]
\centering
\caption{Impact of Evaluation Methodology on Reported Accuracy}
\label{tab:leakage}
\begin{tabular}{lcc}
\toprule
\textbf{Evaluation Level} & \textbf{N} & \textbf{Accuracy (\%)} \\
\midrule
Recording-level (pooled OOF) & 1,931 & 90.9 \\
Subject-level (aggregated) & 153 & 83.7 \\
\midrule
\textbf{Inflation due to leakage} & --- & \textbf{+7.2} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Validation Stability}

Per-fold subject-level accuracy ranged from 74.2\% to 90.0\% across the five folds, with balanced class distributions in each fold confirming successful stratification (Table~\ref{tab:folds}).

\begin{table}[t]
\centering
\caption{Per-Fold Performance (Random Forest)}
\label{tab:folds}
\begin{tabular}{ccccc}
\toprule
\textbf{Fold} & \textbf{Train (C/S)} & \textbf{Test (C/S)} & \textbf{Acc} & \textbf{AUC} \\
\midrule
1 & 62/60 & 15/16 & 0.894 & 0.916 \\
2 & 61/61 & 16/15 & 0.802 & 0.808 \\
3 & 61/61 & 16/15 & 0.743 & 0.773 \\
4 & 62/61 & 15/15 & 0.871 & 0.919 \\
5 & 62/61 & 15/15 & 0.908 & 0.947 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize C = Controls, S = Schizophrenia patients} \\
\end{tabular}
\end{table}

% REVISION: Added cross-validation variance discussion paragraph
\paragraph{Interpreting Cross-Fold Variance.} The 16-point accuracy swing across folds (74.2\%-90.0\%) requires explanation. Three factors contribute to this variance: (1) \textit{Small test set sizes}: each fold contains only 15-16 subjects, where a single misclassification changes accuracy by $\sim$6 percentage points; (2) \textit{Paradigm heterogeneity}: recordings within each subject span multiple EEG paradigms (resting-state, cognitive tasks, MMN, ASSR), and the distribution of paradigms may differ across subjects assigned to different folds; (3) \textit{Expected statistical variance}: with only 30-31 test subjects per fold, binomial sampling variance yields an expected standard deviation of $\sim$7 percentage points at 85\% accuracy. This variance does not indicate model instability but rather reflects the fundamental uncertainty inherent in small-sample clinical classification tasks.

\subsection{Feature Importance Analysis} % REVISION: Removed "Hardware Validation" from title since hardware not validated

Random Forest feature importance analysis (computed using mean decrease in impurity across 300 trees) revealed that frontal channels contributed disproportionately to classification. The top 10 features included: % REVISION: Removed figure reference (fig:importance) and described method inline

\begin{itemize}[leftmargin=*,nosep]
    \item Fp1 theta power (rank 1)
    \item Fp2 alpha power (rank 2)
    \item Fp1 sample entropy (rank 4)
    \item F3 beta power (rank 6)
    \item Fp1-Fp2 coherence (rank 8)
\end{itemize}

The prominence of Fp1 and Fp2 channels provides biological rationale for our proof-of-concept single-channel hardware design targeting the frontal region, though validation with hardware-acquired signals remains future work. Frontal abnormalities are well-documented in schizophrenia, including reduced frontal alpha power and increased theta activity associated with hypofrontality \cite{boutros2008}. % REVISION: Reframed from "validates" to "provides rationale"

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Quantifying the Cost of Methodological Rigor}

The central finding of this work is not the 83.7\% accuracy itself, but rather the \textit{7.2 percentage point gap} between recording-level and subject-level evaluation. This gap quantifies the inflation introduced by identity leakage, a pervasive but often unreported methodological flaw in the EEG-ML literature.

When multiple recordings exist per subject, models can exploit subject-specific artifacts (electrode impedance patterns, head geometry, environmental noise signatures) rather than learning disorder-relevant biomarkers. The resulting accuracy estimates, while numerically impressive, fail to generalize to new individuals, the actual clinical use case.

The apparent ``drop'' in accuracy from 90.9\% to 83.7\% should be reframed as a \textit{scientific correction} rather than a limitation. The subject-level estimate reflects honest generalization performance; the recording-level estimate is an artifact of evaluation methodology. By explicitly reporting both, we enable the field to calibrate expectations and identify studies that may have inadvertently inflated their results.

% REVISION: Added nuance to accuracy gap interpretation
\paragraph{Nuanced Interpretation of the Gap.} We acknowledge that the 7.2 percentage point difference between recording-level and subject-level accuracy cannot be attributed \textit{entirely} to identity leakage. A portion of this gap reflects the legitimate increased difficulty of cross-subject generalization: subjects vary in baseline EEG characteristics, medication regimens, disease severity, and recording conditions. Even without any information leakage, we would expect some performance reduction when evaluating on held-out subjects versus held-out recordings from known subjects. Nevertheless, the magnitude of this gap and its near-universal omission in published studies underscore the importance of honest reporting. Our contribution lies not in perfectly decomposing this gap, but in demonstrating that subject-level evaluation is both feasible and necessary.

\subsection{Clinical Interpretation of the Confusion Matrix}

The confusion matrix (Table~\ref{tab:confusion}) reveals an asymmetric error pattern: the model exhibits high sensitivity (93.4\%) but moderate specificity (74.0\%). This means:

\begin{itemize}[leftmargin=*,nosep]
    \item 71 of 76 patients (93.4\%) are correctly identified
    \item 5 patients are missed (false negatives)
    \item 20 of 77 controls are incorrectly flagged (false positives)
\end{itemize}

For a \textit{screening} tool, this trade-off is clinically appropriate. Missing a schizophrenia case (false negative) carries severe consequences, including delayed treatment, disease progression, and increased risk of adverse outcomes. False positives, while burdensome, can be resolved through subsequent clinical evaluation.

The 74\% specificity suggests that approximately one in four healthy individuals would be incorrectly flagged for follow-up. In a two-stage screening paradigm (EEG screening followed by clinical interview for positive cases), this false positive rate may be acceptable, particularly in high-risk populations where base rates of schizophrenia are elevated.

\subsection{The Case for Uncertainty Quantification}

Beyond point predictions, we advocate for uncertainty quantification in clinical ML systems. Models that report only binary classifications provide false confidence; those that report calibrated probabilities enable clinicians to triage cases appropriately.

In our analysis, prediction probabilities clustered near 0.5 for many subjects, indicating low model confidence. We propose that predictions with probability $<0.6$ or $>0.4$ (within 10 points of the decision boundary) should be flagged as ``uncertain'' and prioritized for clinical review. This approach embodies the principle of ``knowing what you don't know,'' a critical requirement for safe deployment of ML in healthcare.

\subsection{Hardware Implications for Global Health}

Feature importance analysis revealed that frontal channels (Fp1, Fp2) were among the top predictors in our research-grade EEG analysis, providing biological rationale for a single-channel hardware design. This finding has significant implications for accessibility: % REVISION: Changed "validating" to "providing rationale"

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Cost Reduction:} A single-channel system costs approximately \$50, compared to \$5,000-50,000 for clinical-grade 16+ channel systems.

    \item \textbf{Ease of Use:} Single-channel acquisition requires minimal training, enabling potential deployment by community health workers.

    \item \textbf{Biological Plausibility:} Frontal abnormalities are well-established in schizophrenia, including hypofrontality, reduced frontal gamma synchrony, and aberrant prefrontal connectivity \cite{uhlhaas2010}.
\end{enumerate}

Our ESP32-based prototype demonstrates \textit{design} feasibility as a proof-of-concept. All classification results reported in this paper were obtained from research-grade 16-channel EEG equipment. Future work must validate classification performance using hardware-acquired signals, addressing the substantial domain shift between research-grade and low-cost acquisition systems, a non-trivial challenge involving differences in electrode quality, ADC precision, artifact susceptibility, and signal-to-noise ratio. % REVISION: Added explicit caveat about validation status

\subsection{Comparison with Literature}

Published EEG-based schizophrenia classification studies have reported accuracies ranging from 75\% to 99\% \cite{phang2020, murphy2021}. Critical examination reveals that many high-accuracy claims stem from:

\begin{itemize}[leftmargin=*,nosep]
    \item Recording-level evaluation (identity leakage)
    \item Small sample sizes (N < 50 subjects)
    \item Post-hoc model selection
    \item Absence of confidence intervals
\end{itemize}

Our 83.7\% accuracy, while lower than some reported values, is derived from a rigorously validated protocol on a relatively large dataset (N=153). This estimate is more likely to generalize to real-world clinical deployment than inflated metrics from methodologically flawed studies.

% ============================================================================
% 5. LIMITATIONS
% ============================================================================
\section{Limitations}

This work has several limitations:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Single Dataset:} All analyses were conducted on the ASZED-153 dataset. External validation on independent cohorts is essential to confirm generalizability.

    \item \textbf{Geographic Specificity:} The dataset was collected in Nigeria. Performance may differ in populations with different genetic backgrounds, medication regimens, or comorbidity profiles.

    \item \textbf{Hardware Prototype Stage:} The low-cost hardware system was developed as a proof-of-concept. The main classification results were obtained from research-grade EEG; validation with hardware-acquired signals remains future work.

    \item \textbf{Cross-Sectional Design:} This study evaluated diagnostic classification (HC vs. SZ) at a single timepoint. Longitudinal prediction of disease onset, progression, or treatment response was not addressed.

    \item \textbf{Binary Classification:} Schizophrenia is a heterogeneous disorder with multiple subtypes. Binary classification may obscure clinically meaningful subgroup differences.

    \item \textbf{Medication Effects:} Patient recordings were collected during naturalistic treatment. Antipsychotic medications are known to alter EEG patterns, potentially confounding diagnostic biomarkers.
\end{enumerate}

% ============================================================================
% 6. CONCLUSION
% ============================================================================
\section{Conclusion}

This work demonstrates that rigorous methodology (including strict subject-level cross-validation, pre-specified primary analysis, and transparent reporting) yields honest but modest classification performance for EEG-based schizophrenia detection. The 83.7\% subject-level accuracy represents approximately 7 percentage points less than recording-level metrics, quantifying the inflation attributable to identity leakage.

We frame this ``accuracy drop'' as a feature, not a bug. By explicitly correcting for methodological artifacts, we establish a reproducible baseline upon which the field can build. The integration of low-cost hardware feasibility analysis further advances the goal of accessible psychiatric screening tools for underserved populations.

Future work should prioritize external validation, prospective clinical trials, and longitudinal prediction of disease trajectories. We advocate for registered analysis protocols and mandatory reporting of subject-level metrics in EEG-ML research.

In an era of inflated claims and replication failures, this work offers a template for honest, transparent, and clinically grounded AI in psychiatry.

% ============================================================================
% DATA AVAILABILITY
% ============================================================================
\section*{Data and Code Availability} % REVISION: Enhanced reproducibility section

The ASZED-153 dataset is publicly available through Zenodo (DOI: 10.5281/zenodo.14178398).

% REVISION: Added code availability and reproducibility details
\paragraph{Reproducibility Statement.} Complete analysis code is available at \url{https://github.com/[repository]} (to be released upon publication). The analysis environment comprised:
\begin{itemize}[leftmargin=*,nosep]
    \item Python 3.10.12
    \item MNE-Python 1.5.1 (preprocessing)
    \item scikit-learn 1.3.2 (classification)
    \item NumPy 1.24.3, SciPy 1.11.4
    \item Random seed: 42 (used for all cross-validation splits and model initialization)
\end{itemize}
All random number generators were seeded to ensure exact reproducibility of results. The preprocessing and feature extraction pipeline processes the full ASZED dataset in approximately 45 minutes on a standard workstation (Intel i7, 32GB RAM).

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\section*{Acknowledgments}

The authors thank the original ASZED dataset creators for making their data publicly available. We acknowledge the use of computational resources at Indiana University South Bend.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{99}

\bibitem{who2022}
World Health Organization. Schizophrenia Fact Sheet. WHO, 2022.

\bibitem{tandon2013}
Tandon R, Gaebel W, Barch DM, et al. Definition and description of schizophrenia in the DSM-5. \textit{Schizophr Res}. 2013;150(1):3--10.

\bibitem{murphy2021}
Murphy M, Whitton AE, Deccy S, et al. Abnormalities in EEG during sleep and wakefulness in schizophrenia. \textit{JAMA Psychiatry}. 2021;78(9):986--994.

\bibitem{phang2020}
Phang CR, Noman F, Hussain H, Ting CM, Ombao H. A multi-domain connectome convolutional neural network for identifying schizophrenia from EEG connectivity patterns. \textit{IEEE J Biomed Health Inform}. 2020;24(5):1333--1343.

\bibitem{uhlhaas2010}
Uhlhaas PJ, Singer W. Abnormal neural oscillations and synchrony in schizophrenia. \textit{Nat Rev Neurosci}. 2010;11(2):100--113.

\bibitem{roberts2021}
Roberts M, Driggs D, Thorpe M, et al. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. \textit{Nat Mach Intell}. 2021;3(3):199--217.

\bibitem{varoquaux2022}
Varoquaux G, Cheplygina V. Machine learning for medical imaging: methodological failures and recommendations for the future. \textit{NPJ Digit Med}. 2022;5(1):48.

\bibitem{little2017}
Little MA, Varoquaux G, Saeb S, et al. Using and understanding cross-validation strategies. \textit{GigaScience}. 2017;6(5):gix020.

\bibitem{saeb2017}
Saeb S, Lonini L, Jayaraman A, Mohr DC, Kording KP. The need to approximate the use-case in clinical machine learning. \textit{GigaScience}. 2017;6(5):gix019.

\bibitem{aszed2024}
African Schizophrenia EEG Dataset (ASZED). Zenodo. 2024. DOI: 10.5281/zenodo.14178398.

\bibitem{gramfort2013}
Gramfort A, Luessi M, Larson E, et al. MEG and EEG data analysis with MNE-Python. \textit{Front Neurosci}. 2013;7:267.

\bibitem{stam2007}
Stam CJ, Nolte G, Daffertshofer A. Phase lag index: assessment of functional connectivity from multi-channel EEG and MEG with diminished bias from common sources. \textit{Hum Brain Mapp}. 2007;28(11):1178--1193.

\bibitem{kim2000}
Kim DJ, Jeong J, Chae JH, et al. An estimation of the first positive Lyapunov exponent of the EEG in patients with schizophrenia. \textit{Psychiatry Res Neuroimaging}. 2000;98(3):177--189.

\bibitem{pedregosa2011}
Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: Machine learning in Python. \textit{J Mach Learn Res}. 2011;12:2825--2830.

\bibitem{boutros2008}
Boutros NN, Arfken C, Galderisi S, Warrick J, Pratt G, Iacono W. The status of spectral EEG abnormality as a diagnostic test for schizophrenia. \textit{Schizophr Res}. 2008;99(1-3):225--237.

\end{thebibliography}

\end{document}
