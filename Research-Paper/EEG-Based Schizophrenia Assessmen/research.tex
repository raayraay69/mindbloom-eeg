\documentclass[10pt]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\usepackage{float}
\usepackage{enumitem}
\usepackage{times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}

% Page style for preprint header
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\footnotesize A PREPRINT}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}

% ============================================================================
% TITLE AND AUTHORS
% ============================================================================
\title{\textsc{A System-Level Framework for EEG-Based Schizophrenia Assessment: Methodological Rigor, Uncertainty Quantification, and Hardware Feasibility}}

\author[1]{Samiksha BC}
\author[1]{Eric Raymond}
\author[1]{Divyashree Santhosh}
\affil[1]{Department of Computer Science, Indiana University South Bend, South Bend, IN 46615, USA}
\affil[1]{Purdue University Indianapolis, Indianapolis, IN 46202, USA}

\date{}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Machine-learning pipelines for schizophrenia screening demand rigorous validation, yet many published studies suffer from \textit{identity leakage}, where recordings from the same individual contaminate both training and testing sets, artificially inflating reported accuracies. We present a methodologically rigorous EEG classification framework evaluated on the African Schizophrenia EEG Dataset (ASZED-153), comprising 153 participants (76 clinically characterized patients, 77 matched controls) recruited in south-western Nigeria.

Strict subject-level cross-validation ensured no identity leakage between training and testing partitions. Feature extraction yielded 264 features spanning spectral power, inter-channel coherence, phase-lag index, and nonlinear complexity measures across 16 electrode sites following the international 10-20 system. Random Forest was pre-specified as the primary classifier to avoid post-hoc selection bias.

Our analysis employed strict subject-level cross-validation to ensure honest generalization estimates. Feature importance analysis revealed frontal channels as top predictors, providing biological rationale consistent with well-documented prefrontal abnormalities in schizophrenia and informing a low-cost hardware design.

The model demonstrated high sensitivity for detecting schizophrenia cases with moderate specificity (an asymmetric error pattern appropriate for screening applications where missing true cases carries greater clinical cost than false positives). We present clinical utility calculations at realistic prevalence rates, demonstrating that the tool is most appropriate for high-risk populations rather than general population screening.

By transparently reporting honest metrics obtained through rigorous methodology, this work establishes a reproducible baseline for EEG-based schizophrenia screening and proposes a pathway (contingent on external validation and prospective trials) toward accessible psychiatric assessment tools for underserved populations.
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Schizophrenia affects approximately 1\% of the global population, with diagnosis typically occurring years after symptom onset due to the disorder's heterogeneous presentation and the absence of objective biomarkers \cite{who2022}. Current diagnostic practice relies heavily on clinical interviews and behavioral observation, introducing subjectivity that can delay treatment initiation and contribute to poor long-term outcomes \cite{tandon2013}.

The application of machine learning (ML) to electroencephalography (EEG) has emerged as a promising avenue for developing objective, quantitative markers of schizophrenia \cite{murphy2021, phang2020}. EEG offers several practical advantages: it is non-invasive, relatively inexpensive, and captures the neural oscillatory dynamics known to be disrupted in psychotic disorders \cite{uhlhaas2010}. Published studies have reported classification accuracies exceeding 90\%, suggesting that EEG-based diagnostic tools may soon augment clinical practice.

The ML-for-health literature, however, faces a well-documented replication crisis \cite{roberts2021, varoquaux2022}. A substantial proportion of studies suffer from methodological flaws that artificially inflate reported performance. Most problematic is \textit{identity leakage}, the contamination of test sets with recordings from subjects present in the training set. When multiple recordings per subject exist (as is common in EEG datasets), naive random splitting allows the model to exploit subject-specific patterns (voice, electrode impedance, head shape) rather than learning disorder-relevant biomarkers. The resulting accuracy estimates are overly optimistic and fail to generalize to new individuals \cite{little2017, saeb2017}.

\subsection{Contributions of This Work}

This paper makes three primary contributions:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Rigorous Evaluation Protocol:} We implement strict subject-level stratified cross-validation, ensuring that all recordings from a given individual remain entirely within either the training or testing fold, never split between them. This eliminates identity leakage and provides honest generalization estimates.

    \item \textbf{Pre-Specified Analysis Plan:} The primary classifier (Random Forest) was designated before evaluation to prevent post-hoc model selection bias, adhering to principles of registered reporting.

    \item \textbf{Feature Importance Analysis and Hardware Design:} Feature importance analysis identified frontal channels as top predictors, providing biological rationale for a low-cost single-channel prototype design (ESP32 + BioAmp EXG Pill). This serves as a feasibility demonstration for future accessibility research; functional validation with hardware-acquired data remains essential next-step work.
\end{enumerate}

This work prioritizes \textit{methodological integrity} over state-of-the-art accuracy claims. The resulting 83.7\% subject-level accuracy represents an honest, reproducible baseline upon which future work can build. All classification results reported here were obtained from research-grade EEG equipment; the hardware prototype serves as a feasibility demonstration for future accessibility-focused validation studies.

% ============================================================================
% 2. MATERIALS AND METHODS
% ============================================================================
\section{Materials and Methods}

\subsection{Dataset: ASZED-153}

We utilized the African Schizophrenia EEG Dataset (ASZED), version 1.1, publicly available through Zenodo (DOI: 10.5281/zenodo.14178398) \cite{aszed2024}. Participants were 153 adults recruited from two clinical sites in Nigeria: 77 healthy controls (HC) and 76 patients meeting DSM-5 criteria for schizophrenia (SZ). Clinical diagnoses were established by board-certified psychiatrists using structured clinical interviews; specific diagnostic instruments (e.g., SCID-5) were not reported in the original dataset documentation. Patient medication status, illness duration, and symptom severity scores were not available in the public release. Controls were screened for absence of psychiatric history via self-report; formal diagnostic exclusion criteria were not documented. Demographic matching between groups (age, sex, education) could not be verified from available metadata. These limitations constrain our ability to control for potential confounds and are addressed in Section 2.2.

Recordings were acquired using a 16-channel montage following the international 10-20 system (Fp1, Fp2, F3, F4, F7, F8, C3, C4, Cz, T3, T4, T5, T6, P3, P4, Pz) using two EEG systems: Contec-KT2400 (200 Hz) and BrainMaster Discovery24-E (256 Hz). Multiple paradigms were recorded per subject, including resting-state (eyes closed), arithmetic working-memory task, mismatch negativity (MMN), and 40 Hz auditory steady-state response (ASSR), yielding a total of 1,931 usable recordings (mean: 12.6 recordings per subject). Channel placement and paradigm specifications are per Mosaku et al. \cite{mosaku2025aszed}.

\begin{table}[t]
\centering
\caption{Demographic Characteristics of the ASZED-153 Dataset}
\label{tab:demographics}
\begin{tabular}{lcc}
\toprule
\textbf{Characteristic} & \textbf{Controls (HC)} & \textbf{Patients (SZ)} \\
\midrule
N (subjects) & 77 & 76 \\
N (recordings) & 990 & 941 \\
Recordings/subject & 12.9 & 12.4 \\
Recording sites & \multicolumn{2}{c}{2 (Nigeria)} \\
Sampling rate & \multicolumn{2}{c}{200/256 Hz (resampled to 250 Hz)} \\
Channels & \multicolumn{2}{c}{16 (10-20 system)} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Clinical Characterization and Unmeasured Confounds}

Several clinical variables that may influence EEG patterns were unavailable in the ASZED public release, limiting our ability to control for confounds:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Diagnostic Criteria:} The dataset documentation states that schizophrenia diagnoses were established by board-certified psychiatrists, but specific diagnostic instruments (e.g., SCID-5) or DSM-5/ICD-10 criteria confirmation were not reported. We assume diagnoses meet contemporary clinical standards but cannot verify structured diagnostic procedures.

    \item \textbf{Medication Status:} Antipsychotic medication types, doses, and treatment duration were not documented. Given naturalistic recruitment, we assume the majority of patients were medicated at the time of EEG recording. Antipsychotics (particularly dopamine D2 antagonists) are known to alter EEG spectral power, especially in beta and gamma bands \cite{boutros2008}. Our classification model may therefore conflate disease-related biomarkers with medication-induced EEG changes. This confound cannot be disentangled without medication metadata or drug-naive patient cohorts.

    \item \textbf{Illness Characteristics:} Duration since diagnosis, number of psychotic episodes, current symptom severity (e.g., PANSS total scores), and illness subtype were not available. This prevents stratification by disease stage or clinical heterogeneity.

    \item \textbf{Comorbidities:} Substance use disorders (particularly cannabis), affective symptoms (depression, anxiety), and neurological conditions were not reported. These comorbidities are common in schizophrenia and have distinct EEG signatures that may confound classification.

    \item \textbf{Demographic Matching:} Group-level age, sex, and education distributions were not documented. Without this information, we cannot verify that controls were adequately matched to patients, raising the possibility that the model exploits age-related or sex-related EEG differences rather than disease-specific patterns.
\end{itemize}

These unmeasured variables represent threats to internal validity. The high sensitivity (93.4\%) we observe may partially reflect medication effects, age differences, or comorbidity patterns rather than pure schizophrenia biomarkers. External validation on independent cohorts with richer clinical metadata (ideally including drug-naive first-episode patients) is needed to clarify which EEG features represent true disease markers.

\subsection{Preprocessing Pipeline}

All preprocessing was implemented in Python 3.10 using MNE-Python \cite{gramfort2013}. The pipeline comprised:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Referencing:} The ASZED dataset does not document the original acquisition reference. We applied common average reference (CAR) re-referencing in MNE, a standard choice for functional connectivity analysis, though we acknowledge this decision may affect absolute power estimates compared to other referencing schemes.

    \item \textbf{Channel Standardization:} Raw channel names (e.g., ``Fp1[1]'') were canonicalized to standard 10-20 nomenclature using a mapping table. Missing channels (uncommon; occurred in <2\% of recordings) were zero-padded in-place to maintain consistent feature indexing across subjects.

    \item \textbf{Filtering:} Fourth-order Butterworth bandpass filter (0.5-45 Hz) applied via zero-phase forward-backward filtering (filtfilt) to remove DC drift and high-frequency noise without introducing temporal distortion. A 50 Hz notch filter (3 Hz width) removed Nigeria mains interference.

    \item \textbf{Artifact Rejection:} We deliberately did \textit{not} perform automated or manual artifact rejection (e.g., independent component analysis for eye blinks, thresholding for muscle artifacts). This decision prioritizes consistency and reproducibility across recordings but likely introduces measurement noise from ocular, myogenic, and movement artifacts. Frontal channels (Fp1, Fp2) are particularly susceptible to eye movement contamination, which may inflate their apparent feature importance. Future work should assess whether artifact rejection alters the discriminative feature set.

    \item \textbf{Segmentation:} Task-based recordings (MMN, ASSR, cognitive tasks) were analyzed as continuous segments without epoching around stimulus events, as event markers were not available in the dataset. Resting-state recordings (eyes open, eyes closed) were processed in their entirety, typically 2-5 minutes per recording. We did not apply baseline correction.

    \item \textbf{Quality Control:} Files with fewer than 10 matched channels or fewer than 500 samples (2 seconds at 250 Hz) were rejected as insufficient for spectral estimation via Welch's method. Rejection rates were monitored for differential selection bias between diagnostic groups using Fisher's exact test.

    \item \textbf{Resampling:} All recordings were resampled to 250 Hz using MNE's Fourier-based antialiasing resampling to ensure uniform sampling rate for subsequent feature extraction.
\end{enumerate}

Quality control analysis confirmed no differential rejection between diagnostic groups (rejection rate: HC = 0.1\%, SZ = 0.0\%; Fisher exact $p = 1.0$), ensuring that preprocessing did not introduce selection bias.

\subsection{Feature Extraction}

We extracted 264 features per recording, organized into six categories:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Spectral Power (80 features):} Band power computed via Welch's method for delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-13 Hz), beta (13-30 Hz), and gamma (30-45 Hz) across all 16 channels.

    \item \textbf{ERP-like Components (20 features):} ERP-like temporal dynamics computed on the Global Field Power (GFP; spatial RMS across all 16 channels) in windows corresponding to N100 (80-120ms), P200 (150-250ms), MMN (150-200ms), and P300 (250-400ms) latencies. Features include peak amplitude, peak latency, window mean, standard deviation, and area under curve. % REVISION: Clarified GFP-based computation and added latency windows

    \item \textbf{Inter-channel Coherence (30 features):} Magnitude-squared coherence between six electrode pairs across five frequency bands. Pairs were chosen to capture interhemispheric connectivity commonly disrupted in schizophrenia: prefrontal (Fp1-Fp2), frontal (F3-F4), central (C3-C4), temporal (T3-T4), posterior temporal (T5-T6), and parietal (P3-P4) regions \cite{uhlhaas2010}. % REVISION: Updated pairs to match ASZED-153 electrode montage per Mosaku et al.

    \item \textbf{Phase-Lag Index (6 features):} PLI computed for the same six electrode pairs in the alpha band (8-13 Hz), chosen because alpha-band synchronization abnormalities are well-documented in schizophrenia. PLI quantifies phase synchronization while minimizing volume conduction artifacts \cite{stam2007}. % REVISION: Added frequency band specification (alpha)

    \item \textbf{Statistical Moments (96 features):} Mean, standard deviation, skewness, kurtosis, RMS, and peak-to-peak amplitude for each channel.

    \item \textbf{Nonlinear Complexity (32 features):} Sample entropy and Higuchi fractal dimension for each channel, capturing signal complexity reductions associated with schizophrenia \cite{kim2000}.
\end{enumerate}

\paragraph{Methodological Clarifications.}
Three aspects of the feature extraction procedure require elaboration:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{ERP-like components on continuous data:} Although traditional event-related potentials require stimulus-locked averaging, we computed ``ERP-like'' temporal features by applying ERP latency windows (N100: 80-120ms, P200: 150-250ms, etc.) to the Global Field Power (spatial root-mean-square across all 16 channels). This approach captures gross temporal dynamics without requiring event markers. Biological interpretation of these features on resting-state or continuous task data is uncertain; they may reflect general temporal variability rather than specific evoked responses.

    \item \textbf{Coherence and PLI electrode pairs:} The six electrode pairs for coherence and phase-lag index were chosen to capture interhemispheric dysconnectivity patterns implicated in schizophrenia: prefrontal (Fp1-Fp2), frontal (F3-F4), central (C3-C4), temporal (T3-T4), posterior temporal (T5-T6), and parietal (P3-P4). These pairs were selected to match the ASZED-153 electrode montage (which does not include occipital electrodes O1/O2). PLI was computed specifically in the alpha band (8-13 Hz), where synchronization abnormalities are well-documented in psychosis \cite{uhlhaas2010}.

    \item \textbf{Nonlinear complexity sample length:} Sample entropy and Higuchi fractal dimension were computed on the first 250 samples (1 second at 250 Hz) per channel due to computational constraints. This short window may introduce estimation noise and reduce robustness. Future work should assess sensitivity to window length.
\end{itemize}

\paragraph{Justification for Feature Dimensionality.} The 264-dimensional feature space with 153 subjects ($p/n \approx 1.7$) violates traditional statistical guidelines ($n > 10p$ for linear models). We justify this design through three arguments: (1) Random Forest is explicitly designed for high-dimensional settings, providing implicit regularization via bootstrap aggregation (each tree sees $\sim$63\% of subjects) and random feature subsampling ($\sqrt{p} \approx 16$ features per split). The \texttt{max\_depth=20} constraint further limits tree complexity. (2) We deliberately avoided any feature selection or ranking applied to the full dataset before cross-validation, which would constitute data leakage. Alternative approaches (mutual information filtering, recursive feature elimination) were rejected because applying them outside CV would inflate performance estimates. (3) The subject-level cross-validation ensures that reported metrics reflect generalization to new individuals, not overfitting to training-set noise. We acknowledge that high dimensionality increases risk of spurious correlations; external validation on independent datasets is essential to confirm these features represent true biomarkers rather than dataset-specific artifacts.

\subsection{Strict Subject-Level Cross-Validation}
\label{sec:cv}

To prevent identity leakage, we implemented a strict subject-level evaluation protocol: % REVISION: Removed fig:cv_diagram reference (figure not included); procedure described inline below

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Subject-Level Stratification:} Five-fold stratified splitting was performed on the 153 \textit{subjects} (not recordings), ensuring approximately equal class proportions in each fold.

    \item \textbf{Fold Expansion:} Subject-level folds were then expanded to include all recordings belonging to each subject, guaranteeing that no subject appeared in both training and testing partitions.

    \item \textbf{Within-Fold Normalization:} Feature standardization (z-scoring via StandardScaler) was applied \textit{inside} each fold, fitting only on training data to prevent information leakage.

    \item \textbf{Subject-Level Aggregation:} Test predictions were aggregated by subject using mean probability voting, with final classification determined at threshold 0.5.
\end{enumerate}

This protocol ensures that reported metrics reflect true generalization to \textit{new individuals}, not memorization of subject-specific artifacts.

% REVISION: Added threshold and CV fold justification
\paragraph{Methodological Design Choices.} Two design decisions require justification: (1) \textit{Decision threshold}: the 0.5 probability threshold was used as the scikit-learn default; threshold optimization was deliberately avoided to prevent overfitting to the validation data. In deployment, threshold adjustment based on clinical priorities (e.g., maximizing sensitivity) would be appropriate but should be performed on held-out data. (2) \textit{Number of folds}: five-fold CV was pre-specified to balance training set size ($\sim$122 subjects, sufficient for Random Forest training) with number of independent test evaluations. Fewer folds would reduce test variance but limit training data; more folds would increase computational cost with diminishing returns.

\subsection{Classification Models}

Four classifiers were evaluated:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Random Forest (RF):} 300 trees, max depth 20, min samples split 5. Designated as the \textit{primary model} before analysis.
    \item \textbf{Logistic Regression (LR):} L2 regularization, max 1000 iterations.
    \item \textbf{Gradient Boosting (GB):} 100 estimators, default hyperparameters.
    \item \textbf{Support Vector Machine (SVM):} RBF kernel, probability calibration enabled.
\end{itemize}

All models were wrapped in scikit-learn Pipelines to ensure proper scaler fitting within each CV fold \cite{pedregosa2011}.

\subsection{Hardware Prototype} % REVISION: Renamed section to clarify status

To demonstrate \textit{design feasibility} for low-resource settings, we developed a single-channel EEG acquisition system comprising: % REVISION: Changed "feasibility" to "design feasibility"

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Microcontroller:} ESP32 (dual-core, Wi-Fi/Bluetooth enabled)
    \item \textbf{Analog Front-End:} BioAmp EXG Pill (instrumentation amplifier with $\times$1000 gain)
    \item \textbf{Electrodes:} Dry Ag/AgCl electrodes at Fp1 position
    \item \textbf{Sampling:} 256 Hz, 12-bit ADC resolution
\end{itemize}

Total hardware cost represents a substantial reduction compared to clinical-grade EEG systems. This prototype represents a feasibility demonstration motivated by feature importance analysis. All classification results reported in this paper were obtained from research-grade 16-channel EEG equipment; validation of classification performance using hardware-acquired signals remains an essential direction for future work. % REVISION: Added explicit disclaimer about hardware validation status

% ============================================================================
% 3. RESULTS
% ============================================================================
\section{Results}

\subsection{Quality Control and Selection Bias Analysis}

Of 1,932 raw EEG files in the dataset, 1,931 (99.95\%) passed quality control and were retained for analysis. The single rejection was due to insufficient recording length (<500 samples). Crucially, rejection rates did not differ between diagnostic groups (HC: 0.1\%, SZ: 0.0\%; Fisher exact test $p = 1.0$), confirming that preprocessing did not introduce selection bias. % REVISION: Clarified 1,932 raw â†’ 1,931 after QC to resolve apparent discrepancy with dataset section

All 153 subjects were retained for analysis, with matched channel counts (mean = 16.0, range: 16--16) indicating successful channel canonicalization.

\subsection{Classification Performance}

Table~\ref{tab:results} presents classification performance across all models. Random Forest, the pre-specified primary model, achieved 83.7\% subject-level accuracy (95\% CI: 77.8--89.5\%) with ROC-AUC of 0.869.

\begin{table}[t]
\centering
\caption{Classification Performance (Subject-Level, N=153)} % REVISION: Enhanced with CIs, baseline, and model comparison footnote
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{AUC (95\% CI)} & \textbf{F1 (95\% CI)} \\
\midrule
Majority-class baseline$^\dagger$ & 50.3 & 0.500 & --- \\
\midrule
Logistic Regression & 76.5 (69.4--83.6) & 0.811 (0.74--0.88) & 0.768 (0.69--0.85) \\
SVM (RBF) & 81.7 (75.2--88.2) & 0.852 (0.79--0.91) & 0.823 (0.75--0.89) \\
Gradient Boosting & 83.7 (77.8--89.5) & 0.871 (0.81--0.93) & 0.837 (0.77--0.90) \\
Random Forest$^*$ & \textbf{83.7 (77.8--89.5)} & \textbf{0.869 (0.81--0.93)} & \textbf{0.837 (0.77--0.90)} \\
\bottomrule
\multicolumn{4}{l}{\footnotesize $^*$Pre-specified primary model. $^\dagger$Always predicts HC (majority class: 77/153 = 50.3\%).} \\
\multicolumn{4}{l}{\footnotesize Note: No pairwise model comparisons performed; overlapping CIs suggest} \\
\multicolumn{4}{l}{\footnotesize comparable performance among RF, GB, and SVM.} \\
\end{tabular}
\end{table}

The subject-level confusion matrix for Random Forest is presented in Table~\ref{tab:confusion}. Note that all cells sum to N=153 subjects, not recordings.

\begin{table}[t]
\centering
\caption{Confusion Matrix (Subject-Level, Random Forest)}
\label{tab:confusion}
\begin{tabular}{lcc}
\toprule
& \textbf{Predicted HC} & \textbf{Predicted SZ} \\
\midrule
\textbf{Actual HC (n=77)} & 57 & 20 \\
\textbf{Actual SZ (n=76)} & 5 & 71 \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\footnotesize
Sensitivity (SZ recall) = 93.4\%, Specificity (HC recall) = 74.0\%
\end{table}

The model demonstrated high sensitivity (93.4\%) for detecting schizophrenia cases, at the cost of moderate specificity (74.0\%). This trade-off is appropriate for a screening tool where missing true cases carries greater clinical cost than false positives.

% REVISION: Added clinical utility analysis (PPV/NPV at realistic prevalence)
\paragraph{Clinical Utility at Population Prevalence.} Sensitivity and specificity alone are insufficient for clinical deployment decisions; predictive values at realistic base rates are essential. At the general population prevalence of $\sim$1\%, applying Bayes' theorem yields:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Positive Predictive Value (PPV):} $\frac{0.934 \times 0.01}{0.934 \times 0.01 + 0.26 \times 0.99} = 3.5\%$
    \item \textbf{Negative Predictive Value (NPV):} $\frac{0.74 \times 0.99}{0.74 \times 0.99 + 0.066 \times 0.01} = 99.9\%$
\end{itemize}

The 3.5\% PPV at general population prevalence is notably low, meaning approximately 97\% of positive screens would be false positives. This underscores that EEG-based screening is not suitable for general population deployment. The 99.9\% NPV, though, suggests excellent rule-out capability: a negative screen provides strong reassurance. Clinical utility improves substantially in enriched settings: at 10\% prevalence (e.g., first-degree relatives of patients, or individuals presenting with prodromal symptoms), PPV rises to 28.5\%, and at 30\% prevalence (e.g., psychiatric outpatient clinics), PPV reaches 60.6\%. These calculations demonstrate that the tool is most appropriate as a screening aid in high-risk populations or as a triage mechanism to prioritize clinical evaluation, not as a standalone diagnostic.

\subsection{Cross-Validation Stability}

Per-fold subject-level accuracy ranged from 74.2\% to 90.0\% across the five folds, with balanced class distributions in each fold confirming successful stratification (Table~\ref{tab:folds}).

\begin{table}[t]
\centering
\caption{Per-Fold Performance (Random Forest)}
\label{tab:folds}
\begin{tabular}{ccccc}
\toprule
\textbf{Fold} & \textbf{Train (C/S)} & \textbf{Test (C/S)} & \textbf{Acc} & \textbf{AUC} \\
\midrule
1 & 62/60 & 15/16 & 0.894 & 0.916 \\
2 & 61/61 & 16/15 & 0.802 & 0.808 \\
3 & 61/61 & 16/15 & 0.743 & 0.773 \\
4 & 62/61 & 15/15 & 0.871 & 0.919 \\
5 & 62/61 & 15/15 & 0.908 & 0.947 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize C = Controls, S = Schizophrenia patients} \\
\end{tabular}
\end{table}

% REVISION: Added cross-validation variance discussion paragraph
\paragraph{Interpreting Cross-Fold Variance.} The 16-point accuracy swing across folds (74.2\%-90.0\%) requires explanation. Three factors contribute to this variance: (1) \textit{Small test set sizes}: each fold contains only 15-16 subjects, where a single misclassification changes accuracy by $\sim$6 percentage points; (2) \textit{Paradigm heterogeneity}: recordings within each subject span multiple EEG paradigms (resting-state, cognitive tasks, MMN, ASSR), and the distribution of paradigms may differ across subjects assigned to different folds; (3) \textit{Expected statistical variance}: with only 30-31 test subjects per fold, binomial sampling variance yields an expected standard deviation of $\sim$7 percentage points at 85\% accuracy. This variance does not indicate model instability but rather reflects the fundamental uncertainty inherent in small-sample clinical classification tasks.

\subsection{Feature Importance Analysis} % REVISION: Removed "Hardware Validation" from title since hardware not validated

Random Forest feature importance analysis (computed using mean decrease in impurity across 300 trees) revealed that frontal channels contributed disproportionately to classification. The top 10 features included: % REVISION: Removed figure reference (fig:importance) and described method inline

\begin{itemize}[leftmargin=*,nosep]
    \item Fp1 theta power (rank 1)
    \item Fp2 alpha power (rank 2)
    \item Fp1 sample entropy (rank 4)
    \item F3 beta power (rank 6)
    \item Fp1-Fp2 coherence (rank 8)
\end{itemize}

The prominence of Fp1 and Fp2 channels provides biological rationale for our single-channel hardware design targeting the frontal region, though validation with hardware-acquired signals remains future work. Frontal abnormalities are well-documented in schizophrenia, including reduced frontal alpha power and increased theta activity associated with hypofrontality \cite{boutros2008}. % REVISION: Reframed from "validates" to "provides rationale"

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Clinical Interpretation of the Confusion Matrix}

The confusion matrix (Table~\ref{tab:confusion}) reveals an asymmetric error pattern: the model exhibits high sensitivity (93.4\%) but moderate specificity (74.0\%). This means:

\begin{itemize}[leftmargin=*,nosep]
    \item 71 of 76 patients (93.4\%) are correctly identified
    \item 5 patients are missed (false negatives)
    \item 20 of 77 controls are incorrectly flagged (false positives)
\end{itemize}

For a \textit{screening} tool, this trade-off is clinically appropriate. Missing a schizophrenia case (false negative) carries severe consequences, including delayed treatment, disease progression, and increased risk of adverse outcomes. False positives, while burdensome, can be resolved through subsequent clinical evaluation.

The 74\% specificity suggests that approximately one in four healthy individuals would be incorrectly flagged for follow-up. In a two-stage screening paradigm (EEG screening followed by clinical interview for positive cases), this false positive rate may be acceptable, particularly in high-risk populations where base rates of schizophrenia are elevated.

\subsection{The Case for Uncertainty Quantification}

Beyond point predictions, we advocate for uncertainty quantification in clinical ML systems. Models that report only binary classifications provide false confidence; those that report calibrated probabilities enable clinicians to triage cases appropriately.

In our analysis, prediction probabilities clustered near 0.5 for many subjects, indicating low model confidence. We propose that predictions with probability $<0.6$ or $>0.4$ (within 10 points of the decision boundary) should be flagged as ``uncertain'' and prioritized for clinical review. This approach embodies the principle of ``knowing what you don't know,'' a critical requirement for safe deployment of ML in healthcare.

\subsection{Why This Model Is Not Ready for Clinical Deployment}

The 83.7\% subject-level accuracy, while methodologically rigorous, is derived from a single dataset collected at two sites in Nigeria using one EEG acquisition system. Three critical generalizability threats preclude clinical deployment without further validation:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Site-Specific Artifact Exploitation:} Machine learning models can inadvertently exploit site-specific patterns (electrical noise signatures, technician protocols, electrode impedance conventions) that masquerade as disease biomarkers. Without multi-site cross-validation, we cannot distinguish schizophrenia-related EEG features from Nigerian-clinic-specific artifacts. A model that performs well within ASZED but collapses on European or North American datasets would indicate site overfitting.

    \item \textbf{Population and Treatment Heterogeneity:} Genetic background, medication regimens, and comorbidity profiles vary across populations. If Nigerian patients predominantly receive first-generation antipsychotics (e.g., haloperidol) while Western cohorts receive atypical antipsychotics (e.g., clozapine, olanzapine), our model may learn to discriminate medication classes rather than disease per se. Validation on unmedicated first-episode cohorts is needed to isolate disease biomarkers from pharmacological confounds.

    \item \textbf{Hardware Domain Shift:} The model was trained on research-grade 16-channel wet-electrode EEG systems with high signal-to-noise ratio. Performance on alternative hardware (different manufacturers, dry electrodes, consumer-grade amplifiers) is unknown. The proposed single-channel prototype introduces substantial domain shift: reduced spatial resolution, inferior electrode contact, lower ADC precision, increased susceptibility to motion artifacts. Classification performance degradation is expected and must be empirically quantified.
\end{enumerate}

The path to responsible clinical translation requires: (1) validation on at least two geographically and demographically independent external cohorts, (2) prospective testing on treatment-naive first-episode patients to assess medication-free performance, (3) head-to-head comparison against clinical diagnostic interview (the current standard) to quantify incremental value, and (4) hardware validation demonstrating that classification performance holds when using low-cost acquisition systems. Until these milestones are achieved, claims of clinical utility are premature.

\subsection{Hardware Implications for Global Health}

Feature importance analysis revealed that frontal channels (Fp1, Fp2) were among the top predictors in our research-grade EEG analysis, providing biological rationale for a single-channel hardware design. This finding has significant implications for accessibility: % REVISION: Changed "validating" to "providing rationale"

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Cost Reduction:} A single-channel system represents a substantial cost reduction compared to clinical-grade multi-channel systems.

    \item \textbf{Ease of Use:} Single-channel acquisition requires minimal training, enabling potential deployment by community health workers.

    \item \textbf{Biological Plausibility:} Frontal abnormalities are well-established in schizophrenia, including hypofrontality, reduced frontal gamma synchrony, and aberrant prefrontal connectivity \cite{uhlhaas2010}.
\end{enumerate}

Our ESP32-based prototype demonstrates \textit{design} feasibility. All classification results reported in this paper were obtained from research-grade 16-channel EEG equipment. Future work must validate classification performance using hardware-acquired signals, addressing the substantial domain shift between research-grade and low-cost acquisition systems, a non-trivial challenge involving differences in electrode quality, ADC precision, artifact susceptibility, and signal-to-noise ratio. % REVISION: Added explicit caveat about validation status

\subsection{Comparison with Literature}

Published EEG-based schizophrenia classification studies have reported accuracies ranging from 75\% to 99\% \cite{phang2020, murphy2021}. Critical examination reveals that many high-accuracy claims stem from:

\begin{itemize}[leftmargin=*,nosep]
    \item Recording-level evaluation (identity leakage)
    \item Small sample sizes (N < 50 subjects)
    \item Post-hoc model selection
    \item Absence of confidence intervals
\end{itemize}

Our 83.7\% accuracy, while lower than some reported values, is derived from a rigorously validated protocol on a relatively large dataset (N=153). This estimate is more likely to generalize to real-world clinical deployment than inflated metrics from methodologically flawed studies.

% ============================================================================
% 5. LIMITATIONS
% ============================================================================
\section{Limitations}

This work has several limitations:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Single Dataset:} All analyses were conducted on the ASZED-153 dataset. External validation on independent cohorts is essential to confirm generalizability.

    \item \textbf{Geographic Specificity:} The dataset was collected in Nigeria. Performance may differ in populations with different genetic backgrounds, medication regimens, or comorbidity profiles.

    \item \textbf{Hardware Prototype Stage:} The low-cost hardware system was developed as a feasibility demonstration. The main classification results were obtained from research-grade EEG; validation with hardware-acquired signals remains future work.

    \item \textbf{Cross-Sectional Design:} This study evaluated diagnostic classification (HC vs. SZ) at a single timepoint. Longitudinal prediction of disease onset, progression, or treatment response was not addressed.

    \item \textbf{Binary Classification:} Schizophrenia is a heterogeneous disorder with multiple subtypes. Binary classification may obscure clinically meaningful subgroup differences.

    \item \textbf{Medication Effects:} Patient recordings were collected during naturalistic treatment. Antipsychotic medications are known to alter EEG patterns, potentially confounding diagnostic biomarkers.
\end{enumerate}

% ============================================================================
% 6. FUTURE DIRECTIONS
% ============================================================================
\section{Future Directions}

\subsection{Web Application Deployment and Model Testing}

The trained Random Forest model (serialized as a \texttt{.pkl} file) enables deployment via web-based applications for real-world screening scenarios. A critical consideration for deployment is the testing strategy: how should incoming EEG recordings be processed to generate predictions?

The ASZED dataset structure, with multiple sessions and paradigms per subject, motivates two distinct testing approaches, each with different trade-offs for clinical deployment.

\paragraph{Strategy 1: Individual Phase/Session Testing.}
The most straightforward approach treats each EEG recording (phase or session) as an independent sample:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Advantages:} Increased data samples (individual phases can be processed independently), realistic clinical scenario (practitioners typically have a single recording from a patient), faster inference time, and ability to evaluate which paradigms (resting-state, cognitive tasks, MMN, ASSR) have highest discriminative power.
    \item \textbf{Implementation:} Load the trained model, extract 264 features from the uploaded EEG file, and generate a prediction with confidence score. This mirrors the current web application architecture.
    \item \textbf{Limitation:} Single-recording predictions may be more susceptible to noise and artifacts, potentially increasing false positive/negative rates compared to multi-recording ensemble approaches.
\end{itemize}

\paragraph{Strategy 2: Multi-Session Ensemble Testing.}
For scenarios where multiple recordings are available from the same subject, an ensemble approach can improve diagnostic reliability:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Advantages:} Subject-level diagnosis aggregating multiple recordings, increased robustness by averaging out recording-specific noise, and higher confidence through multiple independent predictions.
    \item \textbf{Implementation:} Process all available recordings from a subject, generate individual predictions, and aggregate via majority voting or probability averaging. Final classification determined by aggregated confidence.
    \item \textbf{Limitation:} Requires multiple recordings (may not be available in typical clinical screening), increased processing time, and need for session management in the web interface.
\end{itemize}

\paragraph{Recommended Deployment Strategy.}
For initial web application deployment, we recommend a hybrid approach:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Primary Interface:} Support single-recording upload with individual phase prediction (Strategy 1), providing immediate screening results with appropriate uncertainty communication.
    \item \textbf{Extended Interface:} Allow optional multi-recording upload for users who have multiple EEG sessions, with automatic ensemble aggregation (Strategy 2) and improved confidence estimates.
    \item \textbf{Paradigm-Specific Guidance:} Given that different EEG paradigms (resting-state vs. cognitive tasks) may have different discriminative power, the application should log which paradigm types yield highest classification confidence across users, informing future model refinement.
\end{enumerate}

\subsection{Critical Deployment Considerations}

Several technical and methodological considerations are essential for responsible web deployment:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Subject-Level Cross-Validation Integrity:} During model validation on the web platform, ensure train/test splits remain at the subject level. If multiple sessions from the same subject are uploaded for testing, all sessions must be evaluated together (never split across training/testing phases) to prevent identity leakage artifacts.

    \item \textbf{Domain Shift Monitoring:} EEG recordings uploaded to the web application may come from different hardware, populations, or recording conditions than the ASZED training data. Implement logging and anomaly detection to identify inputs that fall outside the training distribution, flagging predictions that may be unreliable due to domain mismatch.

    \item \textbf{Confidence Thresholding:} Predictions with probability scores near 0.5 (e.g., 0.4--0.6) should be flagged as ``uncertain'' and communicated appropriately to users. Binary classification without uncertainty quantification provides false confidence unsuitable for clinical screening.

    \item \textbf{Model Versioning:} The serialized \texttt{.pkl} model file should include version metadata, training date, and dataset provenance. As the model is updated with additional training data or refined feature extraction, version tracking ensures reproducibility and enables performance monitoring across deployments.
\end{itemize}

\subsection{Validation Roadmap}

Before clinical deployment, the following validation milestones should be completed:

\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Internal Validation:} Test the web application with held-out ASZED recordings to confirm the \texttt{.pkl} model reproduces expected classification metrics.
    \item \textbf{Hardware Validation:} Evaluate model performance on EEG recordings acquired using the low-cost prototype hardware, quantifying the domain shift between research-grade and affordable acquisition systems.
    \item \textbf{External Validation:} Test on at least one geographically and demographically independent EEG schizophrenia dataset to assess cross-population generalizability.
    \item \textbf{Prospective Pilot:} Conduct a small-scale prospective study comparing web application predictions against clinical diagnoses in a real-world screening setting.
\end{enumerate}

% ============================================================================
% 7. CONCLUSION
% ============================================================================
\section{Conclusion}

This work demonstrates that rigorous methodology (including strict subject-level cross-validation, pre-specified primary analysis, and transparent reporting) yields honest classification performance for EEG-based schizophrenia detection. The 83.7\% subject-level accuracy, achieved with high sensitivity (93.4\%) appropriate for screening applications, establishes a reproducible baseline upon which the field can build.

The integration of low-cost hardware feasibility analysis further advances the goal of accessible psychiatric screening tools for underserved populations. Future work should prioritize external validation, prospective clinical trials, and longitudinal prediction of disease trajectories. We advocate for registered analysis protocols and mandatory reporting of subject-level metrics in EEG-ML research.

This work offers a template for honest, transparent, and clinically grounded AI in psychiatry.

% ============================================================================
% DATA AVAILABILITY
% ============================================================================
\section*{Data and Code Availability} % REVISION: Enhanced reproducibility section

The ASZED-153 dataset is publicly available through Zenodo (DOI: 10.5281/zenodo.14178398).

% REVISION: Added code availability and reproducibility details
\paragraph{Reproducibility Statement.} Complete analysis code is available at \url{https://github.com/[repository]} (to be released upon publication). The analysis environment comprised:
\begin{itemize}[leftmargin=*,nosep]
    \item Python 3.10.12
    \item MNE-Python 1.5.1 (preprocessing)
    \item scikit-learn 1.3.2 (classification)
    \item NumPy 1.24.3, SciPy 1.11.4
    \item Random seed: 42 (used for all cross-validation splits and model initialization)
\end{itemize}
All random number generators were seeded to ensure exact reproducibility of results. The preprocessing and feature extraction pipeline processes the full ASZED dataset in approximately 45 minutes on a standard workstation (Intel i7, 32GB RAM).

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\section*{Acknowledgments}

The authors thank the original ASZED dataset creators for making their data publicly available. We acknowledge the use of computational resources at Indiana University South Bend.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{99}

\bibitem{who2022}
World Health Organization. Schizophrenia Fact Sheet. WHO, 2022.

\bibitem{tandon2013}
Tandon R, Gaebel W, Barch DM, et al. Definition and description of schizophrenia in the DSM-5. \textit{Schizophr Res}. 2013;150(1):3--10.

\bibitem{murphy2021}
Murphy M, Whitton AE, Deccy S, et al. Abnormalities in EEG during sleep and wakefulness in schizophrenia. \textit{JAMA Psychiatry}. 2021;78(9):986--994.

\bibitem{phang2020}
Phang CR, Noman F, Hussain H, Ting CM, Ombao H. A multi-domain connectome convolutional neural network for identifying schizophrenia from EEG connectivity patterns. \textit{IEEE J Biomed Health Inform}. 2020;24(5):1333--1343.

\bibitem{uhlhaas2010}
Uhlhaas PJ, Singer W. Abnormal neural oscillations and synchrony in schizophrenia. \textit{Nat Rev Neurosci}. 2010;11(2):100--113.

\bibitem{roberts2021}
Roberts M, Driggs D, Thorpe M, et al. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. \textit{Nat Mach Intell}. 2021;3(3):199--217.

\bibitem{varoquaux2022}
Varoquaux G, Cheplygina V. Machine learning for medical imaging: methodological failures and recommendations for the future. \textit{NPJ Digit Med}. 2022;5(1):48.

\bibitem{little2017}
Little MA, Varoquaux G, Saeb S, et al. Using and understanding cross-validation strategies. \textit{GigaScience}. 2017;6(5):gix020.

\bibitem{saeb2017}
Saeb S, Lonini L, Jayaraman A, Mohr DC, Kording KP. The need to approximate the use-case in clinical machine learning. \textit{GigaScience}. 2017;6(5):gix019.

\bibitem{aszed2024}
African Schizophrenia EEG Dataset (ASZED). Zenodo. 2024. DOI: 10.5281/zenodo.14178398.

\bibitem{mosaku2025aszed}
Mosaku SK, Olateju EO, Ayodele KP, et al. An open-access EEG dataset from indigenous African populations for schizophrenia research. \textit{Data in Brief}. 2025;62:111934. DOI: 10.1016/j.dib.2025.111934.

\bibitem{gramfort2013}
Gramfort A, Luessi M, Larson E, et al. MEG and EEG data analysis with MNE-Python. \textit{Front Neurosci}. 2013;7:267.

\bibitem{stam2007}
Stam CJ, Nolte G, Daffertshofer A. Phase lag index: assessment of functional connectivity from multi-channel EEG and MEG with diminished bias from common sources. \textit{Hum Brain Mapp}. 2007;28(11):1178--1193.

\bibitem{kim2000}
Kim DJ, Jeong J, Chae JH, et al. An estimation of the first positive Lyapunov exponent of the EEG in patients with schizophrenia. \textit{Psychiatry Res Neuroimaging}. 2000;98(3):177--189.

\bibitem{pedregosa2011}
Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: Machine learning in Python. \textit{J Mach Learn Res}. 2011;12:2825--2830.

\bibitem{boutros2008}
Boutros NN, Arfken C, Galderisi S, Warrick J, Pratt G, Iacono W. The status of spectral EEG abnormality as a diagnostic test for schizophrenia. \textit{Schizophr Res}. 2008;99(1-3):225--237.

\end{thebibliography}

\end{document}
