{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASZED EEG Classification Training (v2.3.0)\n",
    "\n",
    "**Dataset:** African Schizophrenia EEG Dataset (ASZED-153)  \n",
    "**Paper DOI:** 10.1016/j.dib.2025.111934  \n",
    "**Data DOI:** 10.5281/zenodo.14178398\n",
    "\n",
    "This notebook trains a Random Forest classifier for schizophrenia detection using EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-924725234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# If this fails, try running again or check for popup authentication window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "# If this fails, try running again or check for popup authentication window\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q mne scipy scikit-learn pandas numpy joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU (optional - not required for Random Forest)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Paths\n",
    "\n",
    "**IMPORTANT:** Update these paths to match your Google Drive structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS FOR YOUR GOOGLE DRIVE (from previous session)\n",
    "# ============================================================\n",
    "\n",
    "# Path to ASZED EDF files\n",
    "ASZED_DATA_PATH = '/content/drive/MyDrive/For Project Schiz/ASZED-153/ASZED/version_1.1'\n",
    "\n",
    "# Path to the CSV spreadsheet with labels\n",
    "CSV_PATH = '/content/drive/MyDrive/For Project Schiz/ASZED-153/ASZED_SpreadSheet.csv'\n",
    "\n",
    "# Output path for results and model\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/For Project Schiz/ASZED-153/results_v230'\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "print(f\"ASZED data path exists: {os.path.exists(ASZED_DATA_PATH)}\")\n",
    "print(f\"CSV path exists: {os.path.exists(CSV_PATH)}\")\n",
    "\n",
    "# List contents to verify\n",
    "if os.path.exists(ASZED_DATA_PATH):\n",
    "    print(f\"\\nContents of {ASZED_DATA_PATH}:\")\n",
    "    !ls -la \"{ASZED_DATA_PATH}\" | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline Code (v2.3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/complete_v230.py\n",
    "\"\"\"\n",
    "ASZED EEG Classification Pipeline (v2.3.0) - Colab Version\n",
    "\n",
    "Authoritative Reference:\n",
    "  Mosaku et al. (2025). \"An open-access EEG dataset from indigenous African\n",
    "  populations for schizophrenia research.\" Data in Brief, 62, 111934.\n",
    "  DOI: 10.1016/j.dib.2025.111934\n",
    "\n",
    "Channel Montage (per Data in Brief paper):\n",
    "  Fp1, Fp2, F3, F4, F7, F8, C3, C4, Cz, T3, T4, T5, T6, P3, P4, Pz\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/tmp\"\n",
    "os.environ[\"NUMBA_DISABLE_CACHING\"] = \"1\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import argparse\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "\n",
    "try:\n",
    "    from scipy.integrate import simpson\n",
    "except ImportError:\n",
    "    from scipy.integrate import simps as simpson\n",
    "\n",
    "try:\n",
    "    from numpy import trapezoid as np_trapz\n",
    "except ImportError:\n",
    "    from numpy import trapz as np_trapz\n",
    "\n",
    "try:\n",
    "    from scipy.stats import fisher_exact\n",
    "    FISHER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FISHER_AVAILABLE = False\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, f1_score,\n",
    ")\n",
    "\n",
    "from joblib import Parallel, delayed, dump\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "N_JOBS = 2  # Colab has 2 CPU cores\n",
    "PRIMARY_MODEL_NAME = \"Random Forest\"\n",
    "\n",
    "# Correct channel montage per Data in Brief paper\n",
    "EXPECTED_CHANNELS = [\n",
    "    \"Fp1\", \"Fp2\", \"F3\", \"F4\", \"F7\", \"F8\", \"C3\", \"C4\",\n",
    "    \"Cz\", \"T3\", \"T4\", \"T5\", \"T6\", \"P3\", \"P4\", \"Pz\"\n",
    "]\n",
    "\n",
    "CHANNEL_ALIASES = {\n",
    "    \"T7\": \"T3\", \"T8\": \"T4\", \"P7\": \"T5\", \"P8\": \"T6\",\n",
    "    \"FP1\": \"Fp1\", \"FP2\": \"Fp2\",\n",
    "}\n",
    "\n",
    "MIN_CHANNELS_REQUIRED = 10\n",
    "ENTROPY_SAMPLE_SIZE = 250\n",
    "\n",
    "\n",
    "def normalize_subject_id(sid) -> str:\n",
    "    try:\n",
    "        if pd.isna(sid):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(sid, (int, np.integer)):\n",
    "        return str(int(sid))\n",
    "    if isinstance(sid, (float, np.floating)):\n",
    "        if float(sid).is_integer():\n",
    "            return str(int(sid))\n",
    "    s = str(sid).strip().lower()\n",
    "    if s in (\"\", \"nan\", \"none\"):\n",
    "        return \"\"\n",
    "    s = re.sub(r\"^subject_\", \"\", s)\n",
    "    if re.fullmatch(r\"\\d+(\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    groups = re.findall(r\"\\d+\", s)\n",
    "    if groups:\n",
    "        return str(int(groups[-1]))\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_column_name(col: str) -> str:\n",
    "    return str(col).strip().lower()\n",
    "\n",
    "\n",
    "def canonicalize_channel_name(ch: str) -> str:\n",
    "    original = str(ch).strip()\n",
    "    result = original\n",
    "    result = re.sub(r\"^EEG[-_ ]?\", \"\", result, flags=re.IGNORECASE)\n",
    "    result = re.sub(r\"\\[\\d+\\]$\", \"\", result)\n",
    "    result = re.sub(r\"[-_ ]+(REF|A1|A2|M1|M2|LE|AVG|CZ)$\", \"\", result, flags=re.IGNORECASE)\n",
    "    result = result.strip(\"-_ \")\n",
    "    if not result:\n",
    "        result = original.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, aszed_dir, csv_path, output_path):\n",
    "        self.ASZED_DIR = Path(aszed_dir)\n",
    "        self.CSV_PATH = Path(csv_path)\n",
    "        self.OUTPUT_PATH = Path(output_path)\n",
    "        self.SAMPLING_RATE = 250\n",
    "        self.N_CHANNELS = 16\n",
    "        self.BANDS = {\n",
    "            \"delta\": (0.5, 4), \"theta\": (4, 8), \"alpha\": (8, 13),\n",
    "            \"beta\": (13, 30), \"gamma\": (30, 45),\n",
    "        }\n",
    "        self.ERP_WINDOWS = {\n",
    "            \"N100\": (20, 30), \"P200\": (37, 62), \"MMN\": (25, 62), \"P300\": (62, 125)\n",
    "        }\n",
    "        self.FILTER_LOW = 0.5\n",
    "        self.FILTER_HIGH = 45\n",
    "        self.NOTCH_FREQ = 50\n",
    "\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_spectral_power(data, fs, bands):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        freqs, psd = signal.welch(data[ch], fs=fs, nperseg=min(256, data.shape[1]))\n",
    "        for low, high in bands.values():\n",
    "            idx = (freqs >= low) & (freqs <= high)\n",
    "            try:\n",
    "                features.append(simpson(psd[idx], x=freqs[idx]) if idx.any() else 0)\n",
    "            except:\n",
    "                features.append(np_trapz(psd[idx], freqs[idx]) if idx.any() else 0)\n",
    "    return np.array(features[:80])\n",
    "\n",
    "\n",
    "def extract_erp_components(data, fs, windows):\n",
    "    features = []\n",
    "    avg = np.mean(data, axis=0)\n",
    "    for comp, (s, e) in windows.items():\n",
    "        if e < len(avg):\n",
    "            w = avg[s:e]\n",
    "            if comp in [\"N100\", \"MMN\"]:\n",
    "                pa, pi = (np.min(w), int(np.argmin(w))) if len(w) else (0, 0)\n",
    "            else:\n",
    "                pa, pi = (np.max(w), int(np.argmax(w))) if len(w) else (0, 0)\n",
    "            features.extend([pa, (s + pi) / fs * 1000 if fs else 0])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "    for s, e in windows.values():\n",
    "        if e < len(avg):\n",
    "            seg = avg[s:e]\n",
    "            features.extend([float(np.mean(seg)), float(np.std(seg)), float(np_trapz(seg))])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])\n",
    "    return np.array(features[:20])\n",
    "\n",
    "\n",
    "def compute_coherence(data, fs, bands):\n",
    "    features = []\n",
    "    pairs = [(0, 1), (2, 3), (6, 7), (9, 10), (13, 14), (11, 12)]\n",
    "    for c1, c2 in pairs:\n",
    "        if np.allclose(data[c1], 0) or np.allclose(data[c2], 0):\n",
    "            features.extend([0.0] * len(bands))\n",
    "            continue\n",
    "        try:\n",
    "            f, coh = signal.coherence(data[c1], data[c2], fs=fs, nperseg=min(256, data.shape[1]))\n",
    "            for low, high in bands.values():\n",
    "                idx = (f >= low) & (f <= high)\n",
    "                features.append(float(np.mean(coh[idx])) if idx.any() else 0.0)\n",
    "        except:\n",
    "            features.extend([0.0] * len(bands))\n",
    "    return np.array(features[:30])\n",
    "\n",
    "\n",
    "def compute_pli(data):\n",
    "    features = []\n",
    "    pairs = [(0, 1), (2, 3), (6, 7), (9, 10), (13, 14), (11, 12)]\n",
    "    for c1, c2 in pairs:\n",
    "        if np.allclose(data[c1], 0) or np.allclose(data[c2], 0):\n",
    "            features.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            a1, a2 = signal.hilbert(data[c1]), signal.hilbert(data[c2])\n",
    "            phase_diff = np.angle(a1) - np.angle(a2)\n",
    "            features.append(float(np.abs(np.mean(np.sign(np.sin(phase_diff))))))\n",
    "        except:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:6])\n",
    "\n",
    "\n",
    "def extract_stats(data):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch]\n",
    "        features.extend([\n",
    "            float(np.mean(d)), float(np.std(d)),\n",
    "            float(stats.skew(d)) if np.std(d) > 0 else 0.0,\n",
    "            float(stats.kurtosis(d)) if np.std(d) > 0 else 0.0,\n",
    "            float(np.sqrt(np.mean(d ** 2))), float(np.ptp(d)),\n",
    "        ])\n",
    "    return np.array(features[:96])\n",
    "\n",
    "\n",
    "def compute_entropy(data, m=2, r=0.2):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch][:ENTROPY_SAMPLE_SIZE] if len(data[ch]) > ENTROPY_SAMPLE_SIZE else data[ch]\n",
    "        if np.std(d) > 0:\n",
    "            d = (d - np.mean(d)) / np.std(d)\n",
    "            N = len(d)\n",
    "            def count_matches(tlen):\n",
    "                count = 0\n",
    "                templates = [d[i:i + tlen] for i in range(N - tlen)]\n",
    "                for i in range(len(templates)):\n",
    "                    for j in range(i + 1, len(templates)):\n",
    "                        if np.max(np.abs(templates[i] - templates[j])) < r:\n",
    "                            count += 1\n",
    "                return count\n",
    "            try:\n",
    "                B, A = count_matches(m), count_matches(m + 1)\n",
    "                features.append(float(-np.log(A / B)) if A > 0 and B > 0 else 0.0)\n",
    "            except:\n",
    "                features.append(0.0)\n",
    "        else:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:16])\n",
    "\n",
    "\n",
    "def compute_fd(data, kmax=10):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch]\n",
    "        N = len(d)\n",
    "        if np.allclose(d, 0) or N <= kmax * 2:\n",
    "            features.append(0.0)\n",
    "            continue\n",
    "        L, x = [], []\n",
    "        for k in range(1, min(kmax + 1, N // 2)):\n",
    "            Lk = 0.0\n",
    "            for m in range(k):\n",
    "                mx = int(np.floor((N - m - 1) / k))\n",
    "                if mx > 0:\n",
    "                    Lmk = 0.0\n",
    "                    for i in range(1, mx + 1):\n",
    "                        i1, i2 = m + i * k, m + (i - 1) * k\n",
    "                        if i1 < N and i2 < N:\n",
    "                            Lmk += np.abs(d[i1] - d[i2])\n",
    "                    Lmk = Lmk * (N - 1) / (mx * k * k)\n",
    "                    Lk += Lmk\n",
    "            if Lk > 0:\n",
    "                L.append(np.log(Lk / k))\n",
    "                x.append(np.log(1.0 / k))\n",
    "        if len(x) > 1:\n",
    "            try:\n",
    "                features.append(float(np.polyfit(x, L, 1)[0]))\n",
    "            except:\n",
    "                features.append(0.0)\n",
    "        else:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:16])\n",
    "\n",
    "\n",
    "def extract_all_features(data, fs, config):\n",
    "    f = []\n",
    "    f.extend(extract_spectral_power(data, fs, config.BANDS))\n",
    "    f.extend(extract_erp_components(data, fs, config.ERP_WINDOWS))\n",
    "    f.extend(compute_coherence(data, fs, config.BANDS))\n",
    "    f.extend(compute_pli(data))\n",
    "    f.extend(extract_stats(data))\n",
    "    f.extend(compute_entropy(data))\n",
    "    f.extend(compute_fd(data))\n",
    "    return np.array(f, dtype=float)\n",
    "\n",
    "\n",
    "def load_labels(csv_path: Path):\n",
    "    print(f\"\\nLoading labels from: {csv_path}\")\n",
    "    for encoding in [\"utf-8-sig\", \"utf-8\", \"latin-1\", \"cp1252\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=encoding, dtype=str)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        raise ValueError(f\"Could not read CSV: {csv_path}\")\n",
    "    \n",
    "    df.columns = [normalize_column_name(c) for c in df.columns]\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    sn_col = None\n",
    "    for c in [\"sn\", \"subject\", \"subject_id\", \"id\"]:\n",
    "        if c in df.columns:\n",
    "            sn_col = c\n",
    "            break\n",
    "    \n",
    "    cat_col = None\n",
    "    for c in [\"category\", \"group\", \"diagnosis\", \"label\"]:\n",
    "        if c in df.columns:\n",
    "            cat_col = c\n",
    "            break\n",
    "    \n",
    "    if not sn_col or not cat_col:\n",
    "        raise ValueError(f\"Missing columns. Found: {list(df.columns)}\")\n",
    "    \n",
    "    label_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sid = normalize_subject_id(row.get(sn_col, \"\"))\n",
    "        if not sid:\n",
    "            continue\n",
    "        cat = str(row.get(cat_col, \"\")).lower().strip()\n",
    "        if \"control\" in cat or \"hc\" in cat or \"healthy\" in cat:\n",
    "            label_map[sid] = 0\n",
    "        elif \"patient\" in cat or \"schiz\" in cat or \"sz\" in cat:\n",
    "            label_map[sid] = 1\n",
    "    \n",
    "    n_ctrl = sum(1 for v in label_map.values() if v == 0)\n",
    "    n_pat = sum(1 for v in label_map.values() if v == 1)\n",
    "    print(f\"  Mapped {len(label_map)} subjects ({n_ctrl} controls, {n_pat} patients)\")\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def find_files(aszed_dir: Path):\n",
    "    print(f\"\\nScanning: {aszed_dir}\")\n",
    "    files = []\n",
    "    for ext in [\"*.edf\", \"*.EDF\", \"*.bdf\", \"*.BDF\"]:\n",
    "        files.extend(aszed_dir.rglob(ext))\n",
    "    print(f\"  Found {len(files)} EEG files\")\n",
    "    \n",
    "    pairs = []\n",
    "    for f in files:\n",
    "        sid = None\n",
    "        for part in f.parts:\n",
    "            part_lower = str(part).lower()\n",
    "            if part_lower.startswith(\"subject_\") or part_lower.startswith(\"sub_\"):\n",
    "                sid = normalize_subject_id(part)\n",
    "                break\n",
    "        if sid:\n",
    "            pairs.append((f, sid))\n",
    "    \n",
    "    print(f\"  Unique subjects: {len(set(s for _, s in pairs))}\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def standardize_to_16ch_matrix(raw, expected_channels, aliases):\n",
    "    raw_to_canonical = {}\n",
    "    seen_canonical = set()\n",
    "    \n",
    "    for ch in raw.ch_names:\n",
    "        base = canonicalize_channel_name(ch)\n",
    "        canonical = None\n",
    "        for alias, canon in aliases.items():\n",
    "            if base.upper() == alias.upper():\n",
    "                canonical = canon\n",
    "                break\n",
    "        if canonical is None:\n",
    "            for exp in expected_channels:\n",
    "                if base.lower() == exp.lower():\n",
    "                    canonical = exp\n",
    "                    break\n",
    "        if canonical and canonical not in seen_canonical:\n",
    "            raw_to_canonical[ch] = canonical\n",
    "            seen_canonical.add(canonical)\n",
    "    \n",
    "    canonical_to_raw = {v: k for k, v in raw_to_canonical.items()}\n",
    "    data = []\n",
    "    channels_found = 0\n",
    "    \n",
    "    for exp_ch in expected_channels:\n",
    "        if exp_ch in canonical_to_raw:\n",
    "            raw_ch = canonical_to_raw[exp_ch]\n",
    "            data.append(raw.get_data(picks=[raw_ch])[0])\n",
    "            channels_found += 1\n",
    "        else:\n",
    "            data.append(np.zeros(raw.n_times, dtype=float))\n",
    "    \n",
    "    return np.vstack(data), channels_found\n",
    "\n",
    "\n",
    "def load_eeg(file_path: Path, target_fs=250):\n",
    "    import mne\n",
    "    mne.set_log_level(\"ERROR\")\n",
    "    \n",
    "    ext = file_path.suffix.lower()\n",
    "    if ext == \".bdf\":\n",
    "        raw = mne.io.read_raw_bdf(str(file_path), preload=True, verbose=\"ERROR\")\n",
    "    else:\n",
    "        raw = mne.io.read_raw_edf(str(file_path), preload=True, verbose=\"ERROR\")\n",
    "    \n",
    "    try:\n",
    "        raw_eeg = raw.copy()\n",
    "        raw_eeg.pick_types(eeg=True, exclude=\"bads\")\n",
    "        if len(raw_eeg.ch_names) > 0:\n",
    "            raw = raw_eeg\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(raw.ch_names) == 0:\n",
    "        raise ValueError(\"No EEG channels found\")\n",
    "    \n",
    "    fs = float(raw.info[\"sfreq\"])\n",
    "    if fs != target_fs:\n",
    "        raw.resample(target_fs)\n",
    "    \n",
    "    data, n_channels = standardize_to_16ch_matrix(raw, EXPECTED_CHANNELS, CHANNEL_ALIASES)\n",
    "    return data, target_fs, n_channels\n",
    "\n",
    "\n",
    "def preprocess(data, fs, config):\n",
    "    out = []\n",
    "    for ch in data:\n",
    "        if np.allclose(ch, 0):\n",
    "            out.append(ch)\n",
    "            continue\n",
    "        ch = ch - np.mean(ch)\n",
    "        try:\n",
    "            nyq = fs / 2.0\n",
    "            low = config.FILTER_LOW / nyq\n",
    "            high = min(config.FILTER_HIGH / nyq, 0.99)\n",
    "            b, a = signal.butter(4, [low, high], \"band\")\n",
    "            ch = signal.filtfilt(b, a, ch)\n",
    "        except:\n",
    "            pass\n",
    "        if config.NOTCH_FREQ < min(config.FILTER_HIGH, fs / 2.0):\n",
    "            try:\n",
    "                b, a = signal.iirnotch(config.NOTCH_FREQ, 30, fs=fs)\n",
    "                ch = signal.filtfilt(b, a, ch)\n",
    "            except:\n",
    "                pass\n",
    "        out.append(ch)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def process_single_file(fp, sid, label_map, config):\n",
    "    import mne\n",
    "    mne.set_log_level(\"ERROR\")\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    label = label_map.get(sid, -1)\n",
    "    try:\n",
    "        if sid not in label_map:\n",
    "            return {\"status\": \"no_label\", \"subject_id\": sid, \"label\": -1}\n",
    "        \n",
    "        data, fs, n_ch = load_eeg(fp, config.SAMPLING_RATE)\n",
    "        \n",
    "        if data.shape[1] < 500:\n",
    "            return {\"status\": \"too_short\", \"subject_id\": sid, \"label\": label}\n",
    "        \n",
    "        if n_ch < MIN_CHANNELS_REQUIRED:\n",
    "            return {\"status\": \"low_channels\", \"subject_id\": sid, \"label\": label}\n",
    "        \n",
    "        data = preprocess(data, fs, config)\n",
    "        feat = extract_all_features(data, fs, config)\n",
    "        feat = np.nan_to_num(feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return {\"status\": \"ok\", \"features\": feat, \"label\": label, \"subject_id\": sid, \"n_channels\": n_ch}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"subject_id\": sid, \"label\": label, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def process_dataset(config, max_files=None):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ASZED-153 PREPROCESSING (v2.3.0)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    label_map = load_labels(config.CSV_PATH)\n",
    "    pairs = find_files(config.ASZED_DIR)\n",
    "    \n",
    "    if not pairs:\n",
    "        raise RuntimeError(\"No EEG files found\")\n",
    "    \n",
    "    if max_files:\n",
    "        pairs = pairs[:max_files]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(pairs)} files...\")\n",
    "    \n",
    "    results = Parallel(n_jobs=N_JOBS, backend=\"loky\")(\n",
    "        delayed(process_single_file)(fp, sid, label_map, config)\n",
    "        for fp, sid in tqdm(pairs, desc=\"Processing\")\n",
    "    )\n",
    "    \n",
    "    results = [r for r in results if isinstance(r, dict)]\n",
    "    valid = [r for r in results if r.get(\"status\") == \"ok\"]\n",
    "    \n",
    "    print(f\"\\n  Accepted: {len(valid)}\")\n",
    "    print(f\"  Rejected: {len(results) - len(valid)}\")\n",
    "    \n",
    "    if not valid:\n",
    "        raise RuntimeError(\"No samples processed successfully\")\n",
    "    \n",
    "    X = np.array([r[\"features\"] for r in valid], dtype=float)\n",
    "    y = np.array([r[\"label\"] for r in valid], dtype=int)\n",
    "    subject_ids = np.array([r[\"subject_id\"] for r in valid], dtype=str)\n",
    "    \n",
    "    n_subjects = len(set(subject_ids))\n",
    "    print(f\"\\n  Recordings: {len(y)} ({(y == 0).sum()} ctrl, {(y == 1).sum()} patient)\")\n",
    "    print(f\"  Subjects: {n_subjects}\")\n",
    "    print(f\"  Features: {X.shape[1]}\")\n",
    "    \n",
    "    return X, y, subject_ids\n",
    "\n",
    "\n",
    "def subject_level_cv(X, y, groups, n_splits=5, random_state=42):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUBJECT-LEVEL CROSS-VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Build subject table\n",
    "    unique_subjects = sorted(set(groups))\n",
    "    subject_labels = {}\n",
    "    for i, subj in enumerate(groups):\n",
    "        if subj not in subject_labels:\n",
    "            subject_labels[subj] = y[i]\n",
    "    \n",
    "    subject_y = np.array([subject_labels[s] for s in unique_subjects])\n",
    "    \n",
    "    # Stratified split on subjects\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Convert subject folds to recording indices\n",
    "    folds = []\n",
    "    for train_subj_idx, test_subj_idx in skf.split(unique_subjects, subject_y):\n",
    "        train_subjects = set(unique_subjects[i] for i in train_subj_idx)\n",
    "        test_subjects = set(unique_subjects[i] for i in test_subj_idx)\n",
    "        train_idx = np.array([i for i, g in enumerate(groups) if g in train_subjects])\n",
    "        test_idx = np.array([i for i, g in enumerate(groups) if g in test_subjects])\n",
    "        folds.append((train_idx, test_idx))\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, random_state=random_state, n_jobs=-1)),\n",
    "    ])\n",
    "    \n",
    "    all_y_true, all_y_pred, all_y_prob, all_subjects = [], [], [], []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(folds):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        all_y_true.extend(y_test.tolist())\n",
    "        all_y_pred.extend(y_pred.tolist())\n",
    "        all_y_prob.extend(y_prob.tolist())\n",
    "        all_subjects.extend(groups[test_idx].tolist())\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"  Fold {fold_idx + 1}: acc={acc:.3f}\")\n",
    "    \n",
    "    # Subject-level aggregation\n",
    "    df = pd.DataFrame({\"subject\": all_subjects, \"y_true\": all_y_true, \"y_prob\": all_y_prob})\n",
    "    agg = df.groupby(\"subject\").agg(y_true=(\"y_true\", \"first\"), y_prob_mean=(\"y_prob\", \"mean\")).reset_index()\n",
    "    agg[\"y_pred\"] = (agg[\"y_prob_mean\"] >= 0.5).astype(int)\n",
    "    \n",
    "    subj_acc = accuracy_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "    rec_acc = accuracy_score(all_y_true, [1 if p >= 0.5 else 0 for p in all_y_prob])\n",
    "    \n",
    "    print(f\"\\n  Recording-level accuracy: {rec_acc*100:.1f}%\")\n",
    "    print(f\"  Subject-level accuracy: {subj_acc*100:.1f}%\")\n",
    "    \n",
    "    return subj_acc, rec_acc, agg\n",
    "\n",
    "\n",
    "def train_final_model(X, y, output_path, random_state=42):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING FINAL MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, random_state=random_state, n_jobs=-1)),\n",
    "    ])\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = output_path / f\"schizophrenia_model_v230_{timestamp}.pkl\"\n",
    "    dump(model, model_path)\n",
    "    \n",
    "    print(f\"\\n  Model saved: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def main(aszed_dir, csv_path, output_path):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASZED EEG Classification Pipeline v2.3.0\")\n",
    "    print(\"Dataset: African Schizophrenia EEG Dataset\")\n",
    "    print(\"  Paper DOI: 10.1016/j.dib.2025.111934\")\n",
    "    print(\"  Data DOI: 10.5281/zenodo.14178398\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = Config(aszed_dir, csv_path, output_path)\n",
    "    \n",
    "    # Process dataset\n",
    "    X, y, subject_ids = process_dataset(config)\n",
    "    \n",
    "    # Cross-validation\n",
    "    subj_acc, rec_acc, agg = subject_level_cv(X, y, subject_ids)\n",
    "    \n",
    "    # Train final model\n",
    "    model_path = train_final_model(X, y, output_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nSubject-level accuracy: {subj_acc*100:.1f}%\")\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    return model_path, subj_acc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) >= 4:\n",
    "        main(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "    else:\n",
    "        print(\"Usage: python complete_v230.py <aszed_dir> <csv_path> <output_path>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run the training\n",
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "\n",
    "from complete_v230 import main\n",
    "\n",
    "# Run training\n",
    "model_path, accuracy = main(\n",
    "    aszed_dir=ASZED_DATA_PATH,\n",
    "    csv_path=CSV_PATH,\n",
    "    output_path=OUTPUT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Copy Model to Backend Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the trained model to a standard location\n",
    "import shutil\n",
    "\n",
    "# Standard model name for backend\n",
    "backend_model_path = f\"{OUTPUT_PATH}/schizophrenia_backend_model.pkl\"\n",
    "shutil.copy(model_path, backend_model_path)\n",
    "\n",
    "print(f\"\\nModel ready for backend: {backend_model_path}\")\n",
    "print(\"\\nDownload this file and place it in: mind-bloom/backend/schizophrenia_backend_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that model loads and works\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "\n",
    "model = load(backend_model_path)\n",
    "\n",
    "# Test with random input (264 features)\n",
    "test_input = np.random.randn(1, 264)\n",
    "prediction = model.predict_proba(test_input)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Test prediction shape: {prediction.shape}\")\n",
    "print(f\"Test output: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
