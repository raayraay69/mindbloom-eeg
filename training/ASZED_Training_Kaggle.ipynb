{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ASZED EEG Classification Training (v2.3.0) - Kaggle\n\n**Dataset:** African Schizophrenia EEG Dataset (ASZED-153)  \n**Paper DOI:** 10.1016/j.dib.2025.111934  \n**Data DOI:** 10.5281/zenodo.14178398\n\nThis notebook automatically downloads the ASZED dataset from Zenodo - no manual upload needed!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q mne zenodo_get"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available resources\n",
    "!nvidia-smi\n",
    "!cat /proc/cpuinfo | grep 'model name' | head -1\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Download ASZED Dataset from Zenodo\n\nThis will download ~2GB of data. Takes about 5-10 minutes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\n\n# Download ASZED from Zenodo (DOI: 10.5281/zenodo.14178398)\nZENODO_RECORD = \"14178398\"\nDOWNLOAD_DIR = \"/kaggle/working/aszed_download\"\n\nos.makedirs(DOWNLOAD_DIR, exist_ok=True)\nos.chdir(DOWNLOAD_DIR)\n\nprint(\"Downloading ASZED-153 dataset from Zenodo...\")\nprint(\"This may take 5-10 minutes for ~2GB of data...\\n\")\n\n!zenodo_get {ZENODO_RECORD}\n\nprint(\"\\nDownload complete! Checking files...\")\n!ls -la"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract zip files if needed\nimport zipfile\nimport glob\n\nos.chdir(DOWNLOAD_DIR)\n\n# Extract any zip files\nfor zf in glob.glob(\"*.zip\"):\n    print(f\"Extracting {zf}...\")\n    with zipfile.ZipFile(zf, 'r') as z:\n        z.extractall(\".\")\n\n# Find the paths\nprint(\"\\nSearching for EDF files and CSV...\")\n!find . -name \"*.edf\" | head -5\n!find . -name \"*.csv\"\n\n# Set paths (adjust based on actual structure)\nASZED_DATA_PATH = \"/kaggle/working/aszed_download/ASZED/version_1.1\"\nCSV_PATH = \"/kaggle/working/aszed_download/ASZED_SpreadSheet.csv\"\nOUTPUT_PATH = \"/kaggle/working\"\n\n# Try alternate paths if needed\nif not os.path.exists(ASZED_DATA_PATH):\n    # Search for version folder\n    for root, dirs, files in os.walk(DOWNLOAD_DIR):\n        if \"version_1.1\" in dirs:\n            ASZED_DATA_PATH = os.path.join(root, \"version_1.1\")\n            break\n        if any(f.endswith('.edf') for f in files):\n            ASZED_DATA_PATH = root\n            break\n\nif not os.path.exists(CSV_PATH):\n    # Search for CSV\n    csv_files = glob.glob(f\"{DOWNLOAD_DIR}/**/*SpreadSheet*.csv\", recursive=True)\n    if csv_files:\n        CSV_PATH = csv_files[0]\n    else:\n        csv_files = glob.glob(f\"{DOWNLOAD_DIR}/**/*.csv\", recursive=True)\n        if csv_files:\n            CSV_PATH = csv_files[0]\n\nprint(f\"\\nASZED_DATA_PATH: {ASZED_DATA_PATH}\")\nprint(f\"CSV_PATH: {CSV_PATH}\")\nprint(f\"OUTPUT_PATH: {OUTPUT_PATH}\")\nprint(f\"\\nData path exists: {os.path.exists(ASZED_DATA_PATH)}\")\nprint(f\"CSV exists: {os.path.exists(CSV_PATH)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Training Pipeline (v2.3.0)\n\nRun this cell to load the training functions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ASZED EEG Classification Pipeline (v2.3.0) - Kaggle Version\n",
    "\n",
    "Authoritative Reference:\n",
    "  Mosaku et al. (2025). \"An open-access EEG dataset from indigenous African\n",
    "  populations for schizophrenia research.\" Data in Brief, 62, 111934.\n",
    "  DOI: 10.1016/j.dib.2025.111934\n",
    "\n",
    "Channel Montage (per Data in Brief paper):\n",
    "  Fp1, Fp2, F3, F4, F7, F8, C3, C4, Cz, T3, T4, T5, T6, P3, P4, Pz\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/tmp\"\n",
    "os.environ[\"NUMBA_DISABLE_CACHING\"] = \"1\"\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "import mne\n",
    "\n",
    "try:\n",
    "    from scipy.integrate import simpson\n",
    "except ImportError:\n",
    "    from scipy.integrate import simps as simpson\n",
    "\n",
    "try:\n",
    "    from numpy import trapezoid as np_trapz\n",
    "except ImportError:\n",
    "    from numpy import trapz as np_trapz\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from joblib import Parallel, delayed, dump\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mne.set_log_level(\"ERROR\")\n",
    "\n",
    "N_JOBS = 4  # Kaggle typically has 4 CPU cores\n",
    "\n",
    "# Correct channel montage per Data in Brief paper\n",
    "EXPECTED_CHANNELS = [\n",
    "    \"Fp1\", \"Fp2\", \"F3\", \"F4\", \"F7\", \"F8\", \"C3\", \"C4\",\n",
    "    \"Cz\", \"T3\", \"T4\", \"T5\", \"T6\", \"P3\", \"P4\", \"Pz\"\n",
    "]\n",
    "\n",
    "CHANNEL_ALIASES = {\n",
    "    \"T7\": \"T3\", \"T8\": \"T4\", \"P7\": \"T5\", \"P8\": \"T6\",\n",
    "    \"FP1\": \"Fp1\", \"FP2\": \"Fp2\",\n",
    "}\n",
    "\n",
    "MIN_CHANNELS_REQUIRED = 10\n",
    "ENTROPY_SAMPLE_SIZE = 250\n",
    "\n",
    "\n",
    "def normalize_subject_id(sid) -> str:\n",
    "    try:\n",
    "        if pd.isna(sid):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(sid, (int, np.integer)):\n",
    "        return str(int(sid))\n",
    "    if isinstance(sid, (float, np.floating)):\n",
    "        if float(sid).is_integer():\n",
    "            return str(int(sid))\n",
    "    s = str(sid).strip().lower()\n",
    "    if s in (\"\", \"nan\", \"none\"):\n",
    "        return \"\"\n",
    "    s = re.sub(r\"^subject_\", \"\", s)\n",
    "    if re.fullmatch(r\"\\d+(\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    groups = re.findall(r\"\\d+\", s)\n",
    "    if groups:\n",
    "        return str(int(groups[-1]))\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_column_name(col: str) -> str:\n",
    "    return str(col).strip().lower()\n",
    "\n",
    "\n",
    "def canonicalize_channel_name(ch: str) -> str:\n",
    "    original = str(ch).strip()\n",
    "    result = original\n",
    "    result = re.sub(r\"^EEG[-_ ]?\", \"\", result, flags=re.IGNORECASE)\n",
    "    result = re.sub(r\"\\[\\d+\\]$\", \"\", result)\n",
    "    result = re.sub(r\"[-_ ]+(REF|A1|A2|M1|M2|LE|AVG|CZ)$\", \"\", result, flags=re.IGNORECASE)\n",
    "    result = result.strip(\"-_ \")\n",
    "    if not result:\n",
    "        result = original.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, aszed_dir, csv_path, output_path):\n",
    "        self.ASZED_DIR = Path(aszed_dir)\n",
    "        self.CSV_PATH = Path(csv_path)\n",
    "        self.OUTPUT_PATH = Path(output_path)\n",
    "        self.SAMPLING_RATE = 250\n",
    "        self.N_CHANNELS = 16\n",
    "        self.BANDS = {\n",
    "            \"delta\": (0.5, 4), \"theta\": (4, 8), \"alpha\": (8, 13),\n",
    "            \"beta\": (13, 30), \"gamma\": (30, 45),\n",
    "        }\n",
    "        self.ERP_WINDOWS = {\n",
    "            \"N100\": (20, 30), \"P200\": (37, 62), \"MMN\": (25, 62), \"P300\": (62, 125)\n",
    "        }\n",
    "        self.FILTER_LOW = 0.5\n",
    "        self.FILTER_HIGH = 45\n",
    "        self.NOTCH_FREQ = 50\n",
    "\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_spectral_power(data, fs, bands):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        freqs, psd = signal.welch(data[ch], fs=fs, nperseg=min(256, data.shape[1]))\n",
    "        for low, high in bands.values():\n",
    "            idx = (freqs >= low) & (freqs <= high)\n",
    "            try:\n",
    "                features.append(simpson(psd[idx], x=freqs[idx]) if idx.any() else 0)\n",
    "            except:\n",
    "                features.append(np_trapz(psd[idx], freqs[idx]) if idx.any() else 0)\n",
    "    return np.array(features[:80])\n",
    "\n",
    "\n",
    "def extract_erp_components(data, fs, windows):\n",
    "    features = []\n",
    "    avg = np.mean(data, axis=0)\n",
    "    for comp, (s, e) in windows.items():\n",
    "        if e < len(avg):\n",
    "            w = avg[s:e]\n",
    "            if comp in [\"N100\", \"MMN\"]:\n",
    "                pa, pi = (np.min(w), int(np.argmin(w))) if len(w) else (0, 0)\n",
    "            else:\n",
    "                pa, pi = (np.max(w), int(np.argmax(w))) if len(w) else (0, 0)\n",
    "            features.extend([pa, (s + pi) / fs * 1000 if fs else 0])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "    for s, e in windows.values():\n",
    "        if e < len(avg):\n",
    "            seg = avg[s:e]\n",
    "            features.extend([float(np.mean(seg)), float(np.std(seg)), float(np_trapz(seg))])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])\n",
    "    return np.array(features[:20])\n",
    "\n",
    "\n",
    "def compute_coherence(data, fs, bands):\n",
    "    features = []\n",
    "    pairs = [(0, 1), (2, 3), (6, 7), (9, 10), (13, 14), (11, 12)]\n",
    "    for c1, c2 in pairs:\n",
    "        if np.allclose(data[c1], 0) or np.allclose(data[c2], 0):\n",
    "            features.extend([0.0] * len(bands))\n",
    "            continue\n",
    "        try:\n",
    "            f, coh = signal.coherence(data[c1], data[c2], fs=fs, nperseg=min(256, data.shape[1]))\n",
    "            for low, high in bands.values():\n",
    "                idx = (f >= low) & (f <= high)\n",
    "                features.append(float(np.mean(coh[idx])) if idx.any() else 0.0)\n",
    "        except:\n",
    "            features.extend([0.0] * len(bands))\n",
    "    return np.array(features[:30])\n",
    "\n",
    "\n",
    "def compute_pli(data):\n",
    "    features = []\n",
    "    pairs = [(0, 1), (2, 3), (6, 7), (9, 10), (13, 14), (11, 12)]\n",
    "    for c1, c2 in pairs:\n",
    "        if np.allclose(data[c1], 0) or np.allclose(data[c2], 0):\n",
    "            features.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            a1, a2 = signal.hilbert(data[c1]), signal.hilbert(data[c2])\n",
    "            phase_diff = np.angle(a1) - np.angle(a2)\n",
    "            features.append(float(np.abs(np.mean(np.sign(np.sin(phase_diff))))))\n",
    "        except:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:6])\n",
    "\n",
    "\n",
    "def extract_stats(data):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch]\n",
    "        features.extend([\n",
    "            float(np.mean(d)), float(np.std(d)),\n",
    "            float(stats.skew(d)) if np.std(d) > 0 else 0.0,\n",
    "            float(stats.kurtosis(d)) if np.std(d) > 0 else 0.0,\n",
    "            float(np.sqrt(np.mean(d ** 2))), float(np.ptp(d)),\n",
    "        ])\n",
    "    return np.array(features[:96])\n",
    "\n",
    "\n",
    "def compute_entropy(data, m=2, r=0.2):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch][:ENTROPY_SAMPLE_SIZE] if len(data[ch]) > ENTROPY_SAMPLE_SIZE else data[ch]\n",
    "        if np.std(d) > 0:\n",
    "            d = (d - np.mean(d)) / np.std(d)\n",
    "            N = len(d)\n",
    "            def count_matches(tlen):\n",
    "                count = 0\n",
    "                templates = [d[i:i + tlen] for i in range(N - tlen)]\n",
    "                for i in range(len(templates)):\n",
    "                    for j in range(i + 1, len(templates)):\n",
    "                        if np.max(np.abs(templates[i] - templates[j])) < r:\n",
    "                            count += 1\n",
    "                return count\n",
    "            try:\n",
    "                B, A = count_matches(m), count_matches(m + 1)\n",
    "                features.append(float(-np.log(A / B)) if A > 0 and B > 0 else 0.0)\n",
    "            except:\n",
    "                features.append(0.0)\n",
    "        else:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:16])\n",
    "\n",
    "\n",
    "def compute_fd(data, kmax=10):\n",
    "    features = []\n",
    "    for ch in range(data.shape[0]):\n",
    "        d = data[ch]\n",
    "        N = len(d)\n",
    "        if np.allclose(d, 0) or N <= kmax * 2:\n",
    "            features.append(0.0)\n",
    "            continue\n",
    "        L, x = [], []\n",
    "        for k in range(1, min(kmax + 1, N // 2)):\n",
    "            Lk = 0.0\n",
    "            for m in range(k):\n",
    "                mx = int(np.floor((N - m - 1) / k))\n",
    "                if mx > 0:\n",
    "                    Lmk = 0.0\n",
    "                    for i in range(1, mx + 1):\n",
    "                        i1, i2 = m + i * k, m + (i - 1) * k\n",
    "                        if i1 < N and i2 < N:\n",
    "                            Lmk += np.abs(d[i1] - d[i2])\n",
    "                    Lmk = Lmk * (N - 1) / (mx * k * k)\n",
    "                    Lk += Lmk\n",
    "            if Lk > 0:\n",
    "                L.append(np.log(Lk / k))\n",
    "                x.append(np.log(1.0 / k))\n",
    "        if len(x) > 1:\n",
    "            try:\n",
    "                features.append(float(np.polyfit(x, L, 1)[0]))\n",
    "            except:\n",
    "                features.append(0.0)\n",
    "        else:\n",
    "            features.append(0.0)\n",
    "    return np.array(features[:16])\n",
    "\n",
    "\n",
    "def extract_all_features(data, fs, config):\n",
    "    f = []\n",
    "    f.extend(extract_spectral_power(data, fs, config.BANDS))\n",
    "    f.extend(extract_erp_components(data, fs, config.ERP_WINDOWS))\n",
    "    f.extend(compute_coherence(data, fs, config.BANDS))\n",
    "    f.extend(compute_pli(data))\n",
    "    f.extend(extract_stats(data))\n",
    "    f.extend(compute_entropy(data))\n",
    "    f.extend(compute_fd(data))\n",
    "    return np.array(f, dtype=float)\n",
    "\n",
    "\n",
    "def load_labels(csv_path: Path):\n",
    "    print(f\"\\nLoading labels from: {csv_path}\")\n",
    "    for encoding in [\"utf-8-sig\", \"utf-8\", \"latin-1\", \"cp1252\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=encoding, dtype=str)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        raise ValueError(f\"Could not read CSV: {csv_path}\")\n",
    "    \n",
    "    df.columns = [normalize_column_name(c) for c in df.columns]\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    sn_col = None\n",
    "    for c in [\"sn\", \"subject\", \"subject_id\", \"id\"]:\n",
    "        if c in df.columns:\n",
    "            sn_col = c\n",
    "            break\n",
    "    \n",
    "    cat_col = None\n",
    "    for c in [\"category\", \"group\", \"diagnosis\", \"label\"]:\n",
    "        if c in df.columns:\n",
    "            cat_col = c\n",
    "            break\n",
    "    \n",
    "    if not sn_col or not cat_col:\n",
    "        raise ValueError(f\"Missing columns. Found: {list(df.columns)}\")\n",
    "    \n",
    "    label_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sid = normalize_subject_id(row.get(sn_col, \"\"))\n",
    "        if not sid:\n",
    "            continue\n",
    "        cat = str(row.get(cat_col, \"\")).lower().strip()\n",
    "        if \"control\" in cat or \"hc\" in cat or \"healthy\" in cat:\n",
    "            label_map[sid] = 0\n",
    "        elif \"patient\" in cat or \"schiz\" in cat or \"sz\" in cat:\n",
    "            label_map[sid] = 1\n",
    "    \n",
    "    n_ctrl = sum(1 for v in label_map.values() if v == 0)\n",
    "    n_pat = sum(1 for v in label_map.values() if v == 1)\n",
    "    print(f\"  Mapped {len(label_map)} subjects ({n_ctrl} controls, {n_pat} patients)\")\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def find_files(aszed_dir: Path):\n",
    "    print(f\"\\nScanning: {aszed_dir}\")\n",
    "    files = []\n",
    "    for ext in [\"*.edf\", \"*.EDF\", \"*.bdf\", \"*.BDF\"]:\n",
    "        files.extend(aszed_dir.rglob(ext))\n",
    "    print(f\"  Found {len(files)} EEG files\")\n",
    "    \n",
    "    pairs = []\n",
    "    for f in files:\n",
    "        sid = None\n",
    "        for part in f.parts:\n",
    "            part_lower = str(part).lower()\n",
    "            if part_lower.startswith(\"subject_\") or part_lower.startswith(\"sub_\"):\n",
    "                sid = normalize_subject_id(part)\n",
    "                break\n",
    "        if sid:\n",
    "            pairs.append((f, sid))\n",
    "    \n",
    "    print(f\"  Unique subjects: {len(set(s for _, s in pairs))}\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def standardize_to_16ch_matrix(raw, expected_channels, aliases):\n",
    "    raw_to_canonical = {}\n",
    "    seen_canonical = set()\n",
    "    \n",
    "    for ch in raw.ch_names:\n",
    "        base = canonicalize_channel_name(ch)\n",
    "        canonical = None\n",
    "        for alias, canon in aliases.items():\n",
    "            if base.upper() == alias.upper():\n",
    "                canonical = canon\n",
    "                break\n",
    "        if canonical is None:\n",
    "            for exp in expected_channels:\n",
    "                if base.lower() == exp.lower():\n",
    "                    canonical = exp\n",
    "                    break\n",
    "        if canonical and canonical not in seen_canonical:\n",
    "            raw_to_canonical[ch] = canonical\n",
    "            seen_canonical.add(canonical)\n",
    "    \n",
    "    canonical_to_raw = {v: k for k, v in raw_to_canonical.items()}\n",
    "    data = []\n",
    "    channels_found = 0\n",
    "    \n",
    "    for exp_ch in expected_channels:\n",
    "        if exp_ch in canonical_to_raw:\n",
    "            raw_ch = canonical_to_raw[exp_ch]\n",
    "            data.append(raw.get_data(picks=[raw_ch])[0])\n",
    "            channels_found += 1\n",
    "        else:\n",
    "            data.append(np.zeros(raw.n_times, dtype=float))\n",
    "    \n",
    "    return np.vstack(data), channels_found\n",
    "\n",
    "\n",
    "def load_eeg(file_path: Path, target_fs=250):\n",
    "    ext = file_path.suffix.lower()\n",
    "    if ext == \".bdf\":\n",
    "        raw = mne.io.read_raw_bdf(str(file_path), preload=True, verbose=\"ERROR\")\n",
    "    else:\n",
    "        raw = mne.io.read_raw_edf(str(file_path), preload=True, verbose=\"ERROR\")\n",
    "    \n",
    "    try:\n",
    "        raw_eeg = raw.copy()\n",
    "        raw_eeg.pick_types(eeg=True, exclude=\"bads\")\n",
    "        if len(raw_eeg.ch_names) > 0:\n",
    "            raw = raw_eeg\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(raw.ch_names) == 0:\n",
    "        raise ValueError(\"No EEG channels found\")\n",
    "    \n",
    "    fs = float(raw.info[\"sfreq\"])\n",
    "    if fs != target_fs:\n",
    "        raw.resample(target_fs)\n",
    "    \n",
    "    data, n_channels = standardize_to_16ch_matrix(raw, EXPECTED_CHANNELS, CHANNEL_ALIASES)\n",
    "    return data, target_fs, n_channels\n",
    "\n",
    "\n",
    "def preprocess(data, fs, config):\n",
    "    out = []\n",
    "    for ch in data:\n",
    "        if np.allclose(ch, 0):\n",
    "            out.append(ch)\n",
    "            continue\n",
    "        ch = ch - np.mean(ch)\n",
    "        try:\n",
    "            nyq = fs / 2.0\n",
    "            low = config.FILTER_LOW / nyq\n",
    "            high = min(config.FILTER_HIGH / nyq, 0.99)\n",
    "            b, a = signal.butter(4, [low, high], \"band\")\n",
    "            ch = signal.filtfilt(b, a, ch)\n",
    "        except:\n",
    "            pass\n",
    "        if config.NOTCH_FREQ < min(config.FILTER_HIGH, fs / 2.0):\n",
    "            try:\n",
    "                b, a = signal.iirnotch(config.NOTCH_FREQ, 30, fs=fs)\n",
    "                ch = signal.filtfilt(b, a, ch)\n",
    "            except:\n",
    "                pass\n",
    "        out.append(ch)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def process_single_file(fp, sid, label_map, config):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    label = label_map.get(sid, -1)\n",
    "    try:\n",
    "        if sid not in label_map:\n",
    "            return {\"status\": \"no_label\", \"subject_id\": sid, \"label\": -1}\n",
    "        \n",
    "        data, fs, n_ch = load_eeg(fp, config.SAMPLING_RATE)\n",
    "        \n",
    "        if data.shape[1] < 500:\n",
    "            return {\"status\": \"too_short\", \"subject_id\": sid, \"label\": label}\n",
    "        \n",
    "        if n_ch < MIN_CHANNELS_REQUIRED:\n",
    "            return {\"status\": \"low_channels\", \"subject_id\": sid, \"label\": label}\n",
    "        \n",
    "        data = preprocess(data, fs, config)\n",
    "        feat = extract_all_features(data, fs, config)\n",
    "        feat = np.nan_to_num(feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return {\"status\": \"ok\", \"features\": feat, \"label\": label, \"subject_id\": sid, \"n_channels\": n_ch}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"subject_id\": sid, \"label\": label, \"error\": str(e)}\n",
    "\n",
    "\n",
    "print(\"Pipeline code loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ASZED-153 PREPROCESSING (v2.3.0)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = Config(ASZED_DATA_PATH, CSV_PATH, OUTPUT_PATH)\n",
    "\n",
    "label_map = load_labels(config.CSV_PATH)\n",
    "pairs = find_files(config.ASZED_DIR)\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No EEG files found - check your paths!\")\n",
    "\n",
    "print(f\"\\nProcessing {len(pairs)} files...\")\n",
    "\n",
    "results = Parallel(n_jobs=N_JOBS, backend=\"loky\")(\n",
    "    delayed(process_single_file)(fp, sid, label_map, config)\n",
    "    for fp, sid in tqdm(pairs, desc=\"Processing\")\n",
    ")\n",
    "\n",
    "results = [r for r in results if isinstance(r, dict)]\n",
    "valid = [r for r in results if r.get(\"status\") == \"ok\"]\n",
    "\n",
    "print(f\"\\n  Accepted: {len(valid)}\")\n",
    "print(f\"  Rejected: {len(results) - len(valid)}\")\n",
    "\n",
    "if not valid:\n",
    "    raise RuntimeError(\"No samples processed successfully\")\n",
    "\n",
    "X = np.array([r[\"features\"] for r in valid], dtype=float)\n",
    "y = np.array([r[\"label\"] for r in valid], dtype=int)\n",
    "subject_ids = np.array([r[\"subject_id\"] for r in valid], dtype=str)\n",
    "\n",
    "n_subjects = len(set(subject_ids))\n",
    "print(f\"\\n  Recordings: {len(y)} ({(y == 0).sum()} ctrl, {(y == 1).sum()} patient)\")\n",
    "print(f\"  Subjects: {n_subjects}\")\n",
    "print(f\"  Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Subject-Level Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBJECT-LEVEL CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_splits = 5\n",
    "random_state = 42\n",
    "\n",
    "# Build subject table\n",
    "unique_subjects = sorted(set(subject_ids))\n",
    "subject_labels = {}\n",
    "for i, subj in enumerate(subject_ids):\n",
    "    if subj not in subject_labels:\n",
    "        subject_labels[subj] = y[i]\n",
    "\n",
    "subject_y = np.array([subject_labels[s] for s in unique_subjects])\n",
    "\n",
    "# Stratified split on subjects\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Convert subject folds to recording indices\n",
    "folds = []\n",
    "for train_subj_idx, test_subj_idx in skf.split(unique_subjects, subject_y):\n",
    "    train_subjects = set(unique_subjects[i] for i in train_subj_idx)\n",
    "    test_subjects = set(unique_subjects[i] for i in test_subj_idx)\n",
    "    train_idx = np.array([i for i, g in enumerate(subject_ids) if g in train_subjects])\n",
    "    test_idx = np.array([i for i, g in enumerate(subject_ids) if g in test_subjects])\n",
    "    folds.append((train_idx, test_idx))\n",
    "\n",
    "all_y_true, all_y_pred, all_y_prob, all_subjects = [], [], [], []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(folds):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, \n",
    "                                        random_state=random_state, n_jobs=-1)),\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    all_y_true.extend(y_test.tolist())\n",
    "    all_y_pred.extend(y_pred.tolist())\n",
    "    all_y_prob.extend(y_prob.tolist())\n",
    "    all_subjects.extend(subject_ids[test_idx].tolist())\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"  Fold {fold_idx + 1}: acc={acc:.3f}\")\n",
    "\n",
    "# Subject-level aggregation\n",
    "df = pd.DataFrame({\"subject\": all_subjects, \"y_true\": all_y_true, \"y_prob\": all_y_prob})\n",
    "agg = df.groupby(\"subject\").agg(y_true=(\"y_true\", \"first\"), y_prob_mean=(\"y_prob\", \"mean\")).reset_index()\n",
    "agg[\"y_pred\"] = (agg[\"y_prob_mean\"] >= 0.5).astype(int)\n",
    "\n",
    "subj_acc = accuracy_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "rec_acc = accuracy_score(all_y_true, [1 if p >= 0.5 else 0 for p in all_y_prob])\n",
    "\n",
    "print(f\"\\n  Recording-level accuracy: {rec_acc*100:.1f}%\")\n",
    "print(f\"  Subject-level accuracy: {subj_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING FINAL MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, \n",
    "                                    random_state=42, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"{OUTPUT_PATH}/schizophrenia_model_v230_{timestamp}.pkl\"\n",
    "dump(final_model, model_path)\n",
    "\n",
    "# Also save with standard backend name\n",
    "backend_model_path = f\"{OUTPUT_PATH}/schizophrenia_backend_model.pkl\"\n",
    "dump(final_model, backend_model_path)\n",
    "\n",
    "print(f\"\\n  Model saved: {model_path}\")\n",
    "print(f\"  Backend model: {backend_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model works\n",
    "from joblib import load\n",
    "\n",
    "test_model = load(backend_model_path)\n",
    "test_input = np.random.randn(1, 264)\n",
    "test_pred = test_model.predict_proba(test_input)\n",
    "\n",
    "print(\"Model verification:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_pred.shape}\")\n",
    "print(f\"  Test prediction: {test_pred}\")\n",
    "print(\"\\n✓ Model works correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "print(\"\\nOutput files (available in 'Output' tab on right):\")\n",
    "!ls -la /kaggle/working/*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nYour model is saved. To download:\n\n1. Click **\"Save Version\"** (top right) → **\"Save & Run All\"**\n2. After it completes, go to your notebook page\n3. Click the **\"Output\"** tab on the right\n4. Download `schizophrenia_backend_model.pkl`\n5. Place it in `mind-bloom/backend/`\n\nOr use Kaggle CLI:\n```\nkaggle kernels output milkandcoding/aszed-training-kaggle -p ./\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}